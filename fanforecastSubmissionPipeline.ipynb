{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial train shape: (230130, 6)\n",
      "Initial test shape: (98550, 5)\n",
      "Min of num_sold BEFORE transform: 5.0\n",
      "Train after transforms: (230130, 15)\n",
      "X_train: (184104, 14) y_train: (184104,)\n",
      "X_val: (46026, 14) y_val: (46026,)\n",
      "[Epoch 1/50] train_loss=0.0042, val_loss=0.0018, MSE=0.0018, RMSE=0.0424, MAE=0.0310, R2=0.9251, MAPE=9471139.84%\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 456\u001b[0m\n\u001b[1;32m    452\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msubmission.csv created with final predictions!\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    455\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m\u001b[38;5;241m==\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m--> 456\u001b[0m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[5], line 335\u001b[0m, in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    333\u001b[0m pred \u001b[38;5;241m=\u001b[39m model(batch_x)\n\u001b[1;32m    334\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(pred, batch_y)\n\u001b[0;32m--> 335\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    336\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m    337\u001b[0m train_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem()\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mlen\u001b[39m(batch_x)\n",
      "File \u001b[0;32m~/Python/FAN-MicroDoppler/.venv/lib/python3.12/site-packages/torch/_tensor.py:581\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    571\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    572\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    573\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    574\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    579\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    580\u001b[0m     )\n\u001b[0;32m--> 581\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    582\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    583\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Python/FAN-MicroDoppler/.venv/lib/python3.12/site-packages/torch/autograd/__init__.py:347\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    342\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    344\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    345\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    346\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 347\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    348\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    349\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    350\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    351\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    352\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    353\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    354\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    355\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Python/FAN-MicroDoppler/.venv/lib/python3.12/site-packages/torch/autograd/graph.py:825\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    823\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[1;32m    824\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 825\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    826\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    827\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    828\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    829\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "##################################\n",
    "# 1) LOAD AND PREPROCESS\n",
    "##################################\n",
    "\n",
    "def load_data():\n",
    "    train = pd.read_csv(\"playground-series-s5e1/train.csv\")\n",
    "    test  = pd.read_csv(\"playground-series-s5e1/test.csv\")\n",
    "    sub   = pd.read_csv(\"playground-series-s5e1/sample_submission.csv\")\n",
    "    return train, test, sub\n",
    "\n",
    "def fill_missing_mean(df):\n",
    "    \"\"\"Fill missing in 'num_sold' with the column mean.\"\"\"\n",
    "    df['num_sold'] = df['num_sold'].fillna(df['num_sold'].mean())\n",
    "    return df\n",
    "\n",
    "def kaggle_transform_num_sold(train):\n",
    "    \"\"\"\n",
    "    1) min–max scale using (x - min) / (max - min)\n",
    "    2) log1p\n",
    "    3) sqrt\n",
    "    4) outlier-clip (IQR)\n",
    "    Returns: transformed train + dictionary of transform stats\n",
    "    \"\"\"\n",
    "\n",
    "    # 1) Compute min, max from the training\n",
    "    num_sold_min = train['num_sold'].min()\n",
    "    num_sold_max = train['num_sold'].max()\n",
    "\n",
    "    # 2) Min–max scale\n",
    "    train['num_sold'] = (train['num_sold'] - num_sold_min) / (num_sold_max - num_sold_min)\n",
    "\n",
    "    # 3) log1p\n",
    "    train['num_sold'] = np.log1p(train['num_sold'])\n",
    "\n",
    "    # 4) sqrt\n",
    "    train['num_sold'] = np.sqrt(train['num_sold'])\n",
    "\n",
    "    # 5) outlier clip with IQR\n",
    "    q1 = train['num_sold'].quantile(0.25)\n",
    "    q3 = train['num_sold'].quantile(0.75)\n",
    "    iqr = q3 - q1\n",
    "    lower = q1 - 1.5*iqr\n",
    "    upper = q3 + 1.5*iqr\n",
    "    train['num_sold'] = train['num_sold'].clip(lower, upper)\n",
    "\n",
    "    # Return transform stats so we can invert later\n",
    "    transform_stats = {\n",
    "        'num_sold_min': num_sold_min,\n",
    "        'num_sold_max': num_sold_max,\n",
    "        'lower_bound': lower,\n",
    "        'upper_bound': upper\n",
    "    }\n",
    "    return train, transform_stats\n",
    "\n",
    "def invert_kaggle_transform(y_pred_transformed, stats):\n",
    "    \"\"\"\n",
    "    Reverse the transformations done by kaggle_transform_num_sold:\n",
    "      1) outlier clip -> we can't truly 'unclip', so we just skip\n",
    "      2) sqrt -> square\n",
    "      3) expm1\n",
    "      4) min–max invert\n",
    "    \"\"\"\n",
    "    # 1) square\n",
    "    y_pred = y_pred_transformed**2\n",
    "\n",
    "    # 2) expm1\n",
    "    y_pred = np.expm1(y_pred)\n",
    "\n",
    "    # 3) invert min–max\n",
    "    num_sold_min = stats['num_sold_min']\n",
    "    num_sold_max = stats['num_sold_max']\n",
    "    y_pred = y_pred * (num_sold_max - num_sold_min) + num_sold_min\n",
    "\n",
    "    return y_pred\n",
    "\n",
    "##################################\n",
    "# 2) CREATE DATASET\n",
    "##################################\n",
    "class Dataset(Dataset):\n",
    "    \"\"\"\n",
    "    We'll treat each row as an independent sample, as the Kaggle example does.\n",
    "    We do one-hot encoding for country, store, product, etc.\n",
    "    X will be all the columns except 'num_sold', y is the transformed 'num_sold'.\n",
    "    \"\"\"\n",
    "    def __init__(self, df):\n",
    "        # df already has 'num_sold' transformed\n",
    "        self.X = df.drop(['num_sold'], axis=1).values.astype(np.float32)\n",
    "        self.y = df['num_sold'].values.astype(np.float32)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return (\n",
    "            torch.tensor(self.X[idx], dtype=torch.float32),\n",
    "            torch.tensor(self.y[idx], dtype=torch.float32)\n",
    "        )\n",
    "\n",
    "##################################\n",
    "# 3) FAN MODEL (Minimal Variation)\n",
    "##################################\n",
    "# We'll reuse your FANTimeSeriesModel, but adapt so it takes X as shape [Batch, Features]\n",
    "# rather than [Batch, seq_len, enc_in]. We'll do a simple fix: treat \"Features\" as \"seq_len\",\n",
    "# or wrap it in a single time step. We'll keep the rest the same just for demonstration.\n",
    "##################################\n",
    "\n",
    "from layers.SelfAttention_Family import FullAttention, AttentionLayer\n",
    "from layers.Transformer_EncDec import Decoder, DecoderLayer, Encoder, EncoderLayer\n",
    "from layers.Embed import DataEmbedding\n",
    "from layers.FANLayer import FANLayer\n",
    "\n",
    "class FAN(nn.Module):\n",
    "    \"\"\"\n",
    "    Minimal example: we pretend we have seq_len=1, enc_in=FeatureDim\n",
    "    to reuse your FAN architecture. \n",
    "    In reality, if you truly want a tabular approach, you might remove \n",
    "    the 1D convolution, etc. \n",
    "    \"\"\"\n",
    "    def __init__(self, feature_dim=20, d_model=128):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "\n",
    "        # We'll treat 'feature_dim' as enc_in\n",
    "        # But we do a single time-step => [B, 1, feature_dim]\n",
    "        # so freq= 'h', etc. => time features = 4 might not truly apply\n",
    "        # We'll do a minimal approach anyway.\n",
    "        class DummyConfigs:\n",
    "            enc_in = feature_dim\n",
    "            dec_in = feature_dim\n",
    "            c_out = 1\n",
    "            d_model = 128\n",
    "            embed = 'timeF'\n",
    "            freq = 'h'\n",
    "            dropout = 0.1\n",
    "            e_layers = 3\n",
    "            d_layers = 1\n",
    "            d_ff = 512\n",
    "            n_heads = 8\n",
    "            factor = 5\n",
    "            activation = 'gelu'\n",
    "            output_attention = False\n",
    "            pred_len = 1\n",
    "            exp_setting = 0\n",
    "        \n",
    "        configs = DummyConfigs()\n",
    "\n",
    "        self.enc_embedding = DataEmbedding(\n",
    "            configs.enc_in, configs.d_model, configs.embed, configs.freq, configs.dropout\n",
    "        )\n",
    "        # We'll reuse the same embedding for dec. \n",
    "        self.dec_embedding = DataEmbedding(\n",
    "            configs.dec_in, configs.d_model, configs.embed, configs.freq, configs.dropout\n",
    "        )\n",
    "\n",
    "        self.fan_layer = FANLayer(configs.d_model, configs.d_model)\n",
    "\n",
    "        # Encoder\n",
    "        self.encoder = Encoder(\n",
    "            [\n",
    "                EncoderLayer(\n",
    "                    AttentionLayer(\n",
    "                        FullAttention(\n",
    "                            False,\n",
    "                            configs.factor,\n",
    "                            attention_dropout=configs.dropout,\n",
    "                            output_attention=configs.output_attention\n",
    "                        ),\n",
    "                        configs.d_model,\n",
    "                        configs.n_heads,\n",
    "                    ),\n",
    "                    configs.d_model,\n",
    "                    configs.d_ff,\n",
    "                    dropout=configs.dropout,\n",
    "                    activation=configs.activation,\n",
    "                    exp_setting=configs.exp_setting\n",
    "                )\n",
    "                for _ in range(configs.e_layers)\n",
    "            ],\n",
    "            norm_layer=nn.LayerNorm(configs.d_model),\n",
    "        )\n",
    "\n",
    "        # Decoder\n",
    "        self.decoder = Decoder(\n",
    "            [\n",
    "                DecoderLayer(\n",
    "                    AttentionLayer(\n",
    "                        FullAttention(\n",
    "                            True,\n",
    "                            configs.factor,\n",
    "                            attention_dropout=configs.dropout,\n",
    "                            output_attention=False,\n",
    "                        ),\n",
    "                        configs.d_model,\n",
    "                        configs.n_heads,\n",
    "                    ),\n",
    "                    AttentionLayer(\n",
    "                        FullAttention(\n",
    "                            False,\n",
    "                            configs.factor,\n",
    "                            attention_dropout=configs.dropout,\n",
    "                            output_attention=False,\n",
    "                        ),\n",
    "                        configs.d_model,\n",
    "                        configs.n_heads,\n",
    "                    ),\n",
    "                    configs.d_model,\n",
    "                    configs.d_ff,\n",
    "                    dropout=configs.dropout,\n",
    "                    activation=configs.activation,\n",
    "                    exp_setting=configs.exp_setting,\n",
    "                )\n",
    "                for _ in range(configs.d_layers)\n",
    "            ],\n",
    "            norm_layer=nn.LayerNorm(configs.d_model),\n",
    "            projection=nn.Linear(configs.d_model, 1, bias=True),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        x: shape [B, feature_dim]\n",
    "        We'll artificially treat it as: [B, 1, feature_dim] to feed the existing code.\n",
    "        We'll also pass a dummy 'time feature' array for x_mark_enc, x_mark_dec.\n",
    "        \"\"\"\n",
    "        B, F = x.shape\n",
    "        # Expand to [B, 1, F]\n",
    "        x_enc = x.unsqueeze(1)\n",
    "        x_dec = x.unsqueeze(1)  # same input for dec\n",
    "        # Create dummy time features [B, 1, 4]\n",
    "        x_mark_enc = torch.zeros(B, 1, 4, device=x.device)\n",
    "        x_mark_dec = torch.zeros(B, 1, 4, device=x.device)\n",
    "\n",
    "        # pass\n",
    "        enc_out = self.enc_embedding(x_enc, x_mark_enc)\n",
    "        enc_out = self.fan_layer(enc_out)\n",
    "        enc_out, _ = self.encoder(enc_out)\n",
    "\n",
    "        dec_out = self.dec_embedding(x_dec, x_mark_dec)\n",
    "        dec_out = self.decoder(dec_out, enc_out)  # shape [B, 1, 1]\n",
    "        return dec_out.squeeze(1).squeeze(-1)  # => [B]\n",
    "\n",
    "##################################\n",
    "# 4) MAIN PIPELINE\n",
    "##################################\n",
    "\n",
    "def main():\n",
    "    #### A) LOAD\n",
    "    train, test, sub = load_data()\n",
    "    print(\"Initial train shape:\", train.shape)\n",
    "    print(\"Initial test shape:\", test.shape)\n",
    "\n",
    "    #### B) Transform train\n",
    "    train = fill_missing_mean(train)\n",
    "    # We now do all the Kaggle-style transforms on 'num_sold'\n",
    "    # remove date, id or keep them if needed, etc.\n",
    "    # We'll just do it like the Kaggle snippet\n",
    "    min_val = train['num_sold'].min()\n",
    "    print(\"Min of num_sold BEFORE transform:\", min_val)\n",
    "\n",
    "    train, transform_stats = kaggle_transform_num_sold(train)\n",
    "\n",
    "    #### C) One-Hot or get_dummies for country, store, product, etc.\n",
    "    # 1) We'll drop 'id','date' if they exist\n",
    "    if 'id' in train.columns: \n",
    "        train.drop('id', axis=1, inplace=True)\n",
    "    if 'date' in train.columns:\n",
    "        train.drop('date', axis=1, inplace=True)\n",
    "\n",
    "    cat_cols = ['country','store','product']\n",
    "    for c in cat_cols:\n",
    "        if c in train.columns:\n",
    "            train = pd.get_dummies(train, columns=[c])\n",
    "\n",
    "    # train now has numeric columns plus 'num_sold'\n",
    "    # We'll separate X,y => y is the transformed 'num_sold'\n",
    "    # e.g. X => everything except 'num_sold'\n",
    "    # y => 'num_sold'\n",
    "    y = train['num_sold'].values\n",
    "    X = train.drop(['num_sold'], axis=1).values.astype(np.float32)\n",
    "\n",
    "    print(\"Train after transforms:\", train.shape)\n",
    "\n",
    "    # D) Build Dataset\n",
    "    # We'll replicate the Kaggle approach: random or time-based train/val?\n",
    "    # For simplicity, let's do a random split:\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    X_train, X_val, y_train, y_val = train_test_split(\n",
    "        X, y, test_size=0.2, shuffle=True, random_state=42\n",
    "    )\n",
    "    print(\"X_train:\", X_train.shape, \"y_train:\", y_train.shape)\n",
    "    print(\"X_val:\", X_val.shape, \"y_val:\", y_val.shape)\n",
    "\n",
    "    # Torch datasets\n",
    "    class SimpleDataset(Dataset):\n",
    "        def __init__(self, X_, y_):\n",
    "            self.X_ = X_\n",
    "            self.y_ = y_\n",
    "        def __len__(self):\n",
    "            return len(self.X_)\n",
    "        def __getitem__(self, idx):\n",
    "            return (\n",
    "                torch.tensor(self.X_[idx], dtype=torch.float32),\n",
    "                torch.tensor(self.y_[idx], dtype=torch.float32)\n",
    "            )\n",
    "\n",
    "    train_ds = SimpleDataset(X_train, y_train)\n",
    "    val_ds   = SimpleDataset(X_val,   y_val)\n",
    "\n",
    "    train_loader = DataLoader(train_ds, batch_size=32, shuffle=True)\n",
    "    val_loader   = DataLoader(val_ds, batch_size=32, shuffle=False)\n",
    "\n",
    "    # E) Initialize your FAN-based model\n",
    "    feature_dim = X_train.shape[1]  # number of columns\n",
    "    model = FAN(feature_dim=feature_dim, d_model=64)\n",
    "    model = model.to('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "    optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "    criterion = nn.MSELoss()\n",
    "\n",
    "    #### F) TRAIN\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    model.train()\n",
    "\n",
    "    epochs = 50\n",
    "    optimizer = optim.Adam(model.parameters(), lr=1e-4)\n",
    "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, factor=0.5, patience=3)\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        for batch_x, batch_y in train_loader:\n",
    "            batch_x, batch_y = batch_x.to(device), batch_y.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            # forward\n",
    "            pred = model(batch_x)\n",
    "            loss = criterion(pred, batch_y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss += loss.item()*len(batch_x)\n",
    "        train_loss /= len(train_ds)\n",
    "\n",
    "        # val\n",
    "        from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "\n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "\n",
    "        val_preds = []\n",
    "        val_trues = []\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for batch_x, batch_y in val_loader:\n",
    "                batch_x, batch_y = batch_x.to(device), batch_y.to(device)\n",
    "                \n",
    "                # Forward pass\n",
    "                vpred = model(batch_x)\n",
    "                \n",
    "                # If your model outputs shape [B,1], flatten it\n",
    "                vpred = vpred.view(-1)\n",
    "                \n",
    "                # Compute MSE loss (or your chosen loss)\n",
    "                vloss = criterion(vpred, batch_y)\n",
    "                val_loss += vloss.item() * len(batch_x)\n",
    "                \n",
    "                # Collect predictions & targets for further metrics\n",
    "                val_preds.append(vpred.cpu().numpy())\n",
    "                val_trues.append(batch_y.cpu().numpy())\n",
    "\n",
    "        val_loss /= len(val_ds)\n",
    "        scheduler.step(val_loss)\n",
    "\n",
    "        # Concatenate all batches\n",
    "        val_preds = np.concatenate(val_preds, axis=0)\n",
    "        val_trues = np.concatenate(val_trues, axis=0)\n",
    "\n",
    "        # Compute additional metrics\n",
    "        mse = mean_squared_error(val_trues, val_preds)\n",
    "        rmse = np.sqrt(mse)\n",
    "        mae = mean_absolute_error(val_trues, val_preds)\n",
    "        r2 = r2_score(val_trues, val_preds)\n",
    "        # MAPE needs care if some y-values are zero; handle gracefully\n",
    "        mape = np.mean(np.abs((val_trues - val_preds) / (val_trues + 1e-8))) * 100\n",
    "\n",
    "        print(f\"[Epoch {epoch+1}/{epochs}] \"\n",
    "            f\"train_loss={train_loss:.4f}, val_loss={val_loss:.4f}, \"\n",
    "            f\"MSE={mse:.4f}, RMSE={rmse:.4f}, MAE={mae:.4f}, R2={r2:.4f}, MAPE={mape:.2f}%\")\n",
    "\n",
    "\n",
    "    #### G) PREDICTION for Test\n",
    "    # We'll do the same transforms to test, then invert at the end\n",
    "    # 1) fill missing if any\n",
    "    #test = fill_missing_mean(test)\n",
    "\n",
    "    # 2) min–max scale => log => sqrt => clip\n",
    "    # BUT test has no 'num_sold' to transform. We'll do a dummy approach, or we might just do final predictions?\n",
    "    # The Kaggle example used an LSTM, so it had real feature engineering. \n",
    "    # Actually we only have 'country','store','product' + 'id','date'.\n",
    "\n",
    "    # We'll replicate the get_dummies approach:\n",
    "    if 'id' in test.columns:\n",
    "        test_id = test['id'].values\n",
    "        test.drop('id', axis=1, inplace=True)\n",
    "    if 'date' in test.columns:\n",
    "        test.drop('date', axis=1, inplace=True)\n",
    "\n",
    "    cat_cols2 = ['country','store','product']\n",
    "    for c in cat_cols2:\n",
    "        if c in test.columns:\n",
    "            test = pd.get_dummies(test, columns=[c])\n",
    "\n",
    "    # Ensure test has the same columns as train (besides 'num_sold')\n",
    "    # If train had columns that test lacks, add them:\n",
    "    train_cols = set(train.drop('num_sold', axis=1).columns)\n",
    "    test_cols  = set(test.columns)\n",
    "    missing_in_test = train_cols - test_cols\n",
    "    for mcol in missing_in_test:\n",
    "        test[mcol] = 0\n",
    "\n",
    "    # Possibly test has extra columns => drop them\n",
    "    extra_in_test = test_cols - train_cols\n",
    "    if len(extra_in_test)>0:\n",
    "        test.drop(list(extra_in_test), axis=1, inplace=True)\n",
    "\n",
    "    # reorder columns to match\n",
    "    train_order = list(train.drop('num_sold',axis=1).columns)\n",
    "    test = test[train_order]\n",
    "\n",
    "    # Convert to float32\n",
    "    X_test = test.values.astype(np.float32)\n",
    "\n",
    "    # Predict \n",
    "    test_dataset = SimpleDataset(X_test, np.zeros(len(X_test), dtype=np.float32))\n",
    "    test_loader  = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "    model.eval()\n",
    "    preds_transformed = []\n",
    "    with torch.no_grad():\n",
    "        for bx, _ in test_loader:\n",
    "            bx = bx.to(device)\n",
    "            out = model(bx)\n",
    "            preds_transformed.append(out.cpu().numpy())\n",
    "    preds_transformed = np.concatenate(preds_transformed, axis=0)  # shape (len(test),)\n",
    "\n",
    "    # Invert the Kaggle transform \n",
    "    # i.e. y_pred_original = (expm1( (y_pred^2) ) * (max-min) ) + min\n",
    "    final_preds = invert_kaggle_transform(preds_transformed, transform_stats)\n",
    "\n",
    "    # Build submission\n",
    "    submission = pd.DataFrame({\n",
    "        'id': test_id,\n",
    "        'num_sold': final_preds\n",
    "    })\n",
    "    submission.to_csv(\"submission2.csv\", index=False)\n",
    "    print(\"submission.csv created with final predictions!\")\n",
    "\n",
    "\n",
    "if __name__==\"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "\n",
      "NVIDIA RTX 5000 Ada Generation Laptop GPU\n",
      "Memory Usage:\n",
      "Allocated: 0.0 GB\n",
      "Cached:    0.1 GB\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print('Using device:', device)\n",
    "print()\n",
    "\n",
    "#Additional Info when using cuda\n",
    "if device.type == 'cuda':\n",
    "    print(torch.cuda.get_device_name(0))\n",
    "    print('Memory Usage:')\n",
    "    print('Allocated:', round(torch.cuda.memory_allocated(0)/1024**3,1), 'GB')\n",
    "    print('Cached:   ', round(torch.cuda.memory_reserved(0)/1024**3,1), 'GB')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
