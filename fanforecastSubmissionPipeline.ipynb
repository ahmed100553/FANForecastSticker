{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import holidays\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import Ridge\n",
    "\n",
    "def compute_decomposition_factors(train_df, test_df, CFG):\n",
    "    \"\"\"\n",
    "    A robust decomposition function that:\n",
    "      1) merges train & test\n",
    "      2) extracts date/time features\n",
    "      3) computes GDP, store, product, weekday factors\n",
    "      4) day-of-year factor via fourier\n",
    "      5) extended sin/cos factor\n",
    "      6) country factor\n",
    "      7) non-periodic day-of-year factor\n",
    "      8) final ratio = product of factors\n",
    "      9) residual (train only)\n",
    "\n",
    "    Missing data in any step => skip/ignore that factor or fill with 1.\n",
    "    \"\"\"\n",
    "    # -------------------------\n",
    "    # 1) Combine train + test\n",
    "    # -------------------------\n",
    "    train_df = train_df.copy()\n",
    "    test_df  = test_df.copy()\n",
    "    train_df['test'] = 0\n",
    "    test_df['test']  = 1\n",
    "    df = pd.concat([train_df, test_df], ignore_index=True)\n",
    "\n",
    "    # -------------------------\n",
    "    # 2) Basic date features\n",
    "    # -------------------------\n",
    "    df['date']      = pd.to_datetime(df['date'])\n",
    "    df['year']      = df['date'].dt.year\n",
    "    df['weekday']   = df['date'].dt.weekday\n",
    "    df['dayofyear'] = df['date'].dt.dayofyear\n",
    "    df['daynum']    = (df['date'] - df['date'].min()).dt.days\n",
    "    df['weeknum']   = df['daynum'] // 7\n",
    "    df['month']     = df['date'].dt.month\n",
    "\n",
    "    # “daysinyear”: needed for sin/cos\n",
    "    # If any year has 0 rows, fill with 1 to avoid division by zero\n",
    "    group_y = df.groupby('year')['id'].count()\n",
    "    group_y = group_y.replace({0:1})  # in case some year has 0 rows\n",
    "    daysinyear = (group_y / (len(CFG.countries) * len(CFG.stores) * len(CFG.products))) \\\n",
    "                 .rename('daysinyear').astype(int).to_frame()\n",
    "    df = df.merge(daysinyear, on='year', how='left')\n",
    "\n",
    "    # Fill any missing daysinyear with median or 1\n",
    "    df['daysinyear'] = df['daysinyear'].fillna(df['daysinyear'].median()).replace({0:1})\n",
    "\n",
    "    # Initialize columns for each factor to 1 by default\n",
    "    df['gdp_factor'] = 1.0\n",
    "    df['store_factor'] = 1.0\n",
    "    df['product_factor'] = 1.0\n",
    "    df['weekday_factor'] = 1.0\n",
    "    df['dayofyear_factor'] = 1.0\n",
    "    df['sincos_factor'] = 1.0\n",
    "    df['country_factor'] = 1.0\n",
    "    df['npdoy_factor'] = 1.0\n",
    "\n",
    "    # ----------------------------------------------------------------\n",
    "    # 3) GDP Factor (country-year)\n",
    "    # ----------------------------------------------------------------\n",
    "    def get_gdp_per_capita(country, year):\n",
    "        url = f\"https://api.worldbank.org/v2/country/{CFG.alpha3[country]}/indicator/NY.GDP.PCAP.CD?date={year}&format=json\"\n",
    "        try:\n",
    "            resp = requests.get(url).json()\n",
    "            return float(resp[1][0]['value'])\n",
    "        except:\n",
    "            return np.nan\n",
    "\n",
    "    gdp_table = {}\n",
    "    for c in CFG.countries:\n",
    "        for y in CFG.years:\n",
    "            gdp_val = get_gdp_per_capita(c, y)\n",
    "            gdp_table[(c, y)] = gdp_val\n",
    "\n",
    "    for row_i in df.index:\n",
    "        c = df.at[row_i, 'country']\n",
    "        y = df.at[row_i, 'year']\n",
    "        val = gdp_table.get((c,y), np.nan)\n",
    "        if pd.notna(val):\n",
    "            df.at[row_i, 'gdp_factor'] = val\n",
    "\n",
    "    # Fill remaining NaNs in gdp_factor with median\n",
    "    gdp_median = df['gdp_factor'].median(skipna=True)\n",
    "    df['gdp_factor'] = df['gdp_factor'].fillna(gdp_median)\n",
    "\n",
    "    # ----------------------------------------------------------------\n",
    "    # 4) Store Factor\n",
    "    # ----------------------------------------------------------------\n",
    "    # Exclude countries with known issues (Canada, Kenya). Then group.\n",
    "    mask_valid = ~df['country'].isin(['Canada','Kenya']) & (df['test'] == 0)\n",
    "    store_mean = df[mask_valid].groupby('store')['num_sold'].mean()\n",
    "    # fill with global store mean if missing\n",
    "    store_mean = store_mean.fillna(store_mean.median())\n",
    "\n",
    "    for s, val in store_mean.items():\n",
    "        df.loc[df['store'] == s, 'store_factor'] = val\n",
    "    # Normalize store_factor: e.g. divide by median so typical store=1\n",
    "    med_store = df['store_factor'].median(skipna=True)\n",
    "    df['store_factor'] = df['store_factor'].fillna(med_store).apply(lambda x: x/med_store)\n",
    "\n",
    "    # ----------------------------------------------------------------\n",
    "    # 5) Product Factor (Ridge on sin/cos)\n",
    "    # ----------------------------------------------------------------\n",
    "    df['partofyear']  = (df['dayofyear'] - 1) / df['daysinyear']\n",
    "    df['partof2year'] = df['partofyear'] + (df['year'] % 2)\n",
    "\n",
    "    df['sin t']   = np.sin(2*np.pi * df['partofyear'])\n",
    "    df['cos t']   = np.cos(2*np.pi * df['partofyear'])\n",
    "    df['sin t/2'] = np.sin(np.pi * df['partof2year'])\n",
    "    df['cos t/2'] = np.cos(np.pi * df['partof2year'])\n",
    "\n",
    "    sincos_cols = ['sin t','cos t','sin t/2','cos t/2']\n",
    "\n",
    "    # We'll only compute product_factor for train rows & valid countries\n",
    "    # Then fill test w/predicted or fallback=1 if not possible\n",
    "    df_prod = df[(df['test'] == 0) & (~df['country'].isin(['Canada','Kenya']))].copy()\n",
    "    # total sold by date => for ratio\n",
    "    tot_by_date = df_prod.groupby('date')['num_sold'].sum().rename('daily_sum')\n",
    "    df_prod = df_prod.merge(tot_by_date, on='date', how='left')\n",
    "    df_prod['num_sold_ratio'] = df_prod['num_sold'] / df_prod['daily_sum']\n",
    "\n",
    "    for prod in CFG.products:\n",
    "        sub = df_prod[df_prod['product'] == prod].copy()\n",
    "        if sub.empty:\n",
    "            continue\n",
    "        # X => sin/cos\n",
    "        X = sub[sincos_cols].values\n",
    "        y = sub['num_sold_ratio'].values\n",
    "\n",
    "        # Drop NaN rows\n",
    "        valid = (~np.isnan(X).any(axis=1)) & (~np.isnan(y))\n",
    "        X = X[valid]\n",
    "        y = y[valid]\n",
    "        if len(X) < 2:  # not enough data to fit\n",
    "            continue\n",
    "\n",
    "        # Fit\n",
    "        reg = Ridge()\n",
    "        try:\n",
    "            reg.fit(X,y)\n",
    "        except ValueError:\n",
    "            continue\n",
    "\n",
    "        # Predict for all rows with this product\n",
    "        mask_prod = (df['product'] == prod)\n",
    "        X_pred = df.loc[mask_prod, sincos_cols].fillna(0).values\n",
    "        df.loc[mask_prod, 'product_factor'] = reg.predict(X_pred)\n",
    "\n",
    "    # product_factor => typical scale ~1\n",
    "    # So we normalize by median\n",
    "    med_pf = df['product_factor'].median(skipna=True)\n",
    "    if pd.notna(med_pf) and med_pf!=0:\n",
    "        df['product_factor'] = df['product_factor'].fillna(med_pf).div(med_pf)\n",
    "    else:\n",
    "        df['product_factor'] = 1.0  # fallback\n",
    "\n",
    "    # ----------------------------------------------------------------\n",
    "    # 6) Weekday Factor\n",
    "    # ----------------------------------------------------------------\n",
    "    # We'll compute a median ratio by weekday\n",
    "    # ignoring holiday. We'll define holiday below.\n",
    "    df['holiday'] = 0\n",
    "    for c in CFG.countries:\n",
    "        c_hols = holidays.CountryHoliday(CFG.countries_2l[c], years=CFG.years)\n",
    "        hdays  = [pd.Timestamp(str(d)) for d in c_hols.keys()]\n",
    "        mask_c = (df['country'] == c) & (df['date'].isin(hdays))\n",
    "        df.loc[mask_c, 'holiday'] = 1\n",
    "\n",
    "    # If you want to exclude holiday entirely:\n",
    "    no_hol_mask = (df['holiday'] == 0) & (df['test'] == 0)\n",
    "    weekday_ratio = df[no_hol_mask].groupby('weekday')['num_sold'].sum()\n",
    "    total_sum = weekday_ratio.sum()\n",
    "    if total_sum > 0:\n",
    "        weekday_ratio = weekday_ratio / total_sum\n",
    "        # apply to df\n",
    "        for w, val in weekday_ratio.items():\n",
    "            df.loc[df['weekday'] == w, 'weekday_factor'] = val\n",
    "    # Ensure no NaN\n",
    "    df['weekday_factor'] = df['weekday_factor'].fillna(df['weekday_factor'].median())\n",
    "\n",
    "    # ----------------------------------------------------------------\n",
    "    # 7) Day-of-year Factor (Fourier)\n",
    "    # ----------------------------------------------------------------\n",
    "    # partial_ratio => gdp*store*product*weekday\n",
    "    df['partial_ratio'] = (\n",
    "        df['gdp_factor'] *\n",
    "        df['store_factor'] *\n",
    "        df['product_factor'] *\n",
    "        df['weekday_factor']\n",
    "    )\n",
    "    # avoid div-zero\n",
    "    df['partial_ratio'] = df['partial_ratio'].replace({0:np.nan})\n",
    "    df['temp_total'] = df['num_sold'] / df['partial_ratio']\n",
    "\n",
    "    # exclude holidays\n",
    "    df['holiday_response'] = 0\n",
    "    for c in CFG.countries:\n",
    "        c_hols = holidays.CountryHoliday(CFG.countries_2l[c], years=CFG.years)\n",
    "        for hol_day, _ in c_hols.items():\n",
    "            rng = pd.date_range(hol_day, periods=CFG.holiday_response_len)\n",
    "            mask_hr = (df['country']==c) & (df['date'].isin(rng))\n",
    "            df.loc[mask_hr, 'holiday_response'] = 1\n",
    "\n",
    "    # dayofyear => median of temp_total\n",
    "    sub_doy = df[(df['holiday_response']==0) & (df['test']==0)]\n",
    "    dayofyear_median = sub_doy.groupby('dayofyear')['temp_total'].median()\n",
    "    dayofyear_median = dayofyear_median.dropna()\n",
    "    if dayofyear_median.empty:\n",
    "        # skip dayofyear_factor\n",
    "        df['dayofyear_factor'] = 1.0\n",
    "    else:\n",
    "        xvals = dayofyear_median.index.values\n",
    "        yvals = dayofyear_median.values\n",
    "        # Basic 1-frequency Fourier\n",
    "        def fourier_func(t):\n",
    "            return np.vstack([np.sin(2*np.pi/365.0 * t),\n",
    "                              np.cos(2*np.pi/365.0 * t)]).T\n",
    "        # Filter out NaN\n",
    "        valid = (~np.isnan(xvals)) & (~np.isnan(yvals))\n",
    "        xvals = xvals[valid]\n",
    "        yvals = yvals[valid]\n",
    "        if len(xvals)>2:\n",
    "            reg_doy = Ridge(alpha=0.01)\n",
    "            try:\n",
    "                reg_doy.fit(fourier_func(xvals), yvals)\n",
    "                # predict for t=1..366\n",
    "                t_seq = np.arange(1,367)\n",
    "                yoy_pred = reg_doy.predict(fourier_func(t_seq))\n",
    "                # map to df\n",
    "                yoy_map = dict(zip(t_seq, yoy_pred))\n",
    "                df['dayofyear_factor'] = df['dayofyear'].map(yoy_map).fillna(1.0)\n",
    "            except ValueError:\n",
    "                df['dayofyear_factor'] = 1.0\n",
    "        else:\n",
    "            df['dayofyear_factor'] = 1.0\n",
    "\n",
    "    # ----------------------------------------------------------------\n",
    "    # 8) Extended sincos factor\n",
    "    # ----------------------------------------------------------------\n",
    "    df['sin 2t'] = np.sin(4*np.pi*df['partofyear'])\n",
    "    df['cos 2t'] = np.cos(4*np.pi*df['partofyear'])\n",
    "    df['sin 3t'] = np.sin(6*np.pi*df['partofyear'])\n",
    "    df['cos 3t'] = np.cos(6*np.pi*df['partofyear'])\n",
    "    df['sin 4t'] = np.sin(8*np.pi*df['partofyear'])\n",
    "    df['cos 4t'] = np.cos(8*np.pi*df['partofyear'])\n",
    "    sincoscol2 = ['sin 4t','cos 4t','sin 3t','cos 3t','sin 2t','cos 2t','sin t','cos t','sin t/2','cos t/2']\n",
    "\n",
    "    # median of temp_total by date => ridges\n",
    "    sub2 = df[(df['test']==0) & (df['holiday_response']==0)].copy()\n",
    "    daily_median = sub2.groupby('date')['temp_total'].median().rename('median')\n",
    "    # join sincos\n",
    "    sc_merged = pd.merge(\n",
    "        df[['date']+sincoscol2].drop_duplicates(),\n",
    "        daily_median, on='date', how='left'\n",
    "    )\n",
    "    sc_merged = sc_merged.dropna(subset=['median'])\n",
    "    if len(sc_merged)>2:\n",
    "        X_sc = sc_merged[sincoscol2].values\n",
    "        y_sc = sc_merged['median'].values\n",
    "        valid2 = (~np.isnan(X_sc).any(axis=1)) & (~np.isnan(y_sc))\n",
    "        X_sc = X_sc[valid2]\n",
    "        y_sc = y_sc[valid2]\n",
    "        if len(X_sc)>2:\n",
    "            reg_sc = Ridge(alpha=0.01, fit_intercept=True)\n",
    "            try:\n",
    "                reg_sc.fit(X_sc, y_sc)\n",
    "                # compute sincos_factor for entire df\n",
    "                mat = df[sincoscol2].fillna(0).values\n",
    "                sc_pred = reg_sc.intercept_ + np.sum(mat * reg_sc.coef_, axis=1)\n",
    "                df['sincos_factor'] = sc_pred\n",
    "            except ValueError:\n",
    "                pass\n",
    "\n",
    "    # normalize sincos_factor\n",
    "    med_sf = df['sincos_factor'].median(skipna=True)\n",
    "    if pd.notna(med_sf) and med_sf!=0:\n",
    "        df['sincos_factor'] = df['sincos_factor'].fillna(med_sf) / med_sf\n",
    "    else:\n",
    "        df['sincos_factor'] = 1.0\n",
    "\n",
    "    # ----------------------------------------------------------------\n",
    "    # 9) Country factor\n",
    "    # ----------------------------------------------------------------\n",
    "    # partial ratio (no country factor yet)\n",
    "    partial_ratio2 = (\n",
    "        df['gdp_factor'] *\n",
    "        df['store_factor'] *\n",
    "        df['product_factor'] *\n",
    "        df['weekday_factor'] *\n",
    "        df['sincos_factor'] *\n",
    "        df['dayofyear_factor']\n",
    "    ).replace({0:np.nan})\n",
    "    df['temp_total2'] = df['num_sold'] / partial_ratio2\n",
    "\n",
    "    # e.g. pick product='Kaggle Mug'\n",
    "    mask_kagg = (df['test']==0) & (df['product']=='Kaggle Mug')\n",
    "    if mask_kagg.sum()>0:\n",
    "        c_sums = df[mask_kagg].groupby('country')['temp_total2'].sum().rename('country_factor')\n",
    "        c_sums = c_sums.dropna()\n",
    "        if not c_sums.empty and c_sums.median()!=0:\n",
    "            c_sums = c_sums / c_sums.median()\n",
    "            for c_, val_ in c_sums.items():\n",
    "                df.loc[df['country']==c_, 'country_factor'] = val_\n",
    "\n",
    "    # fill missing country_factor => 1\n",
    "    df['country_factor'] = df['country_factor'].fillna(1.0)\n",
    "\n",
    "    # ----------------------------------------------------------------\n",
    "    # 10) Non-periodic day-of-year factor\n",
    "    # ----------------------------------------------------------------\n",
    "    part3 = (\n",
    "        df['gdp_factor'] *\n",
    "        df['store_factor'] *\n",
    "        df['product_factor'] *\n",
    "        df['weekday_factor'] *\n",
    "        df['sincos_factor'] *\n",
    "        df['country_factor'] *\n",
    "        df['dayofyear_factor']\n",
    "    ).replace({0:np.nan})\n",
    "    df['temp_total3'] = df['num_sold']/part3\n",
    "    group_npdoy = df[df['test']==0].groupby('dayofyear')['temp_total3'].median()\n",
    "    group_npdoy = group_npdoy.dropna()\n",
    "    if not group_npdoy.empty:\n",
    "        # map\n",
    "        for d_oy, val_ in group_npdoy.items():\n",
    "            df.loc[df['dayofyear']==d_oy, 'npdoy_factor'] = val_/group_npdoy.median()\n",
    "    # fill missing => 1\n",
    "    df['npdoy_factor'] = df['npdoy_factor'].fillna(1.0)\n",
    "\n",
    "    # ----------------------------------------------------------------\n",
    "    # 11) Final ratio\n",
    "    # ----------------------------------------------------------------\n",
    "    df['ratio'] = (\n",
    "        df['gdp_factor'] *\n",
    "        df['store_factor'] *\n",
    "        df['product_factor'] *\n",
    "        df['weekday_factor'] *\n",
    "        df['dayofyear_factor'] *\n",
    "        df['sincos_factor'] *\n",
    "        df['country_factor'] *\n",
    "        df['npdoy_factor']\n",
    "    ).replace({0:np.nan})\n",
    "    df['ratio'] = df['ratio'].fillna(1.0)\n",
    "\n",
    "    # ----------------------------------------------------------------\n",
    "    # 12) residual\n",
    "    # ----------------------------------------------------------------\n",
    "    train_mask = (df['test']==0)\n",
    "    df.loc[train_mask, 'residual'] = df.loc[train_mask, 'num_sold'] / df.loc[train_mask, 'ratio']\n",
    "    df.loc[~train_mask, 'residual'] = np.nan  # test => unknown\n",
    "\n",
    "    # Drop intermediate columns if you like\n",
    "    drop_cols = ['daysinyear','holiday','holiday_response','temp_total','temp_total2','temp_total3']\n",
    "    for col in drop_cols:\n",
    "        if col in df.columns:\n",
    "            df.drop(col, axis=1, inplace=True, errors='ignore')\n",
    "\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training samples: 221259\n",
      "[Epoch 1/2] MSE Loss: 68.263383\n",
      "[Epoch 2/2] MSE Loss: 68.216184\n",
      "Saved submission.csv!\n",
      "       id  num_sold\n",
      "0  230130   -952856\n",
      "1  230131   -952856\n",
      "2  230132   -952856\n",
      "3  230133   -952856\n",
      "4  230134   -952856\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Example config (matching your article's approach)\n",
    "class CFG:\n",
    "    countries = [\"Finland\", \"Canada\", \"Italy\", \"Kenya\", \"Singapore\", \"Norway\"]\n",
    "    stores = [\"KaggleMart\", \"KaggleRama\"]  # example\n",
    "    products = [\"Kaggle Mug\", \"Kaggle Hat\", \"Kaggle Sticker\"]  # example\n",
    "    years_train = [2015, 2016, 2017]\n",
    "    years_test = [2018]\n",
    "    years = years_train + years_test\n",
    "    alpha3 = {\"Finland\":\"FIN\",\"Canada\":\"CAN\",\"Italy\":\"IT\",\"Kenya\":\"KEN\",\"Singapore\":\"SGP\",\"Norway\":\"NOR\"}\n",
    "    countries_2l = {\"Finland\": \"FI\", \"Canada\": \"CA\", \"Italy\": \"IT\", \"Kenya\": \"KE\", \"Singapore\": \"SG\", \"Norway\": \"NO\"}\n",
    "    # The columns used for product factor\n",
    "    sincoscol = [\"sin t\",\"cos t\",\"sin t/2\",\"cos t/2\"]\n",
    "    holiday_response_len = 3  # days\n",
    "\n",
    "\n",
    "###############################\n",
    "# 1) Factor computation\n",
    "###############################\n",
    "from functools import partial\n",
    "# Suppose we've put the function compute_decomposition_factors in a separate .py\n",
    "# For simplicity, we define it inline or you can import it.\n",
    "compute_factors = compute_decomposition_factors\n",
    "\n",
    "###############################\n",
    "# 2) FAN definition\n",
    "###############################\n",
    "# We'll use your minimal FAN for demonstration.\n",
    "from layers.SelfAttention_Family import FullAttention, AttentionLayer\n",
    "from layers.Transformer_EncDec import Decoder, DecoderLayer, Encoder, EncoderLayer\n",
    "from layers.Embed import DataEmbedding\n",
    "from layers.FANLayer import FANLayer\n",
    "\n",
    "class FANModel(nn.Module):\n",
    "    \"\"\"Minimal demo: single-step seq, reusing your FAN logic.\"\"\"\n",
    "    def __init__(self, feature_dim=8, d_model=64):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "\n",
    "        class DummyConfigs:\n",
    "            enc_in = feature_dim\n",
    "            dec_in = feature_dim\n",
    "            c_out = 1\n",
    "            d_model = 64\n",
    "            embed = 'timeF'\n",
    "            freq = 'h'\n",
    "            dropout = 0.1\n",
    "            e_layers = 3\n",
    "            d_layers = 1\n",
    "            d_ff = 128\n",
    "            n_heads = 4\n",
    "            factor = 5\n",
    "            activation = 'gelu'\n",
    "            output_attention = False\n",
    "            pred_len = 1\n",
    "            exp_setting = 0\n",
    "\n",
    "        configs = DummyConfigs()\n",
    "\n",
    "        self.enc_embedding = DataEmbedding(\n",
    "            configs.enc_in, configs.d_model, configs.embed, configs.freq, configs.dropout\n",
    "        )\n",
    "        self.dec_embedding = DataEmbedding(\n",
    "            configs.dec_in, configs.d_model, configs.embed, configs.freq, configs.dropout\n",
    "        )\n",
    "\n",
    "        self.fan_layer = FANLayer(configs.d_model, configs.d_model)\n",
    "\n",
    "        # Encoder\n",
    "        self.encoder = Encoder(\n",
    "            [\n",
    "                EncoderLayer(\n",
    "                    AttentionLayer(\n",
    "                        FullAttention(\n",
    "                            False,\n",
    "                            configs.factor,\n",
    "                            attention_dropout=configs.dropout,\n",
    "                            output_attention=configs.output_attention\n",
    "                        ),\n",
    "                        configs.d_model,\n",
    "                        configs.n_heads,\n",
    "                    ),\n",
    "                    configs.d_model,\n",
    "                    configs.d_ff,\n",
    "                    dropout=configs.dropout,\n",
    "                    activation=configs.activation,\n",
    "                    exp_setting=configs.exp_setting\n",
    "                )\n",
    "                for _ in range(configs.e_layers)\n",
    "            ],\n",
    "            norm_layer=nn.LayerNorm(configs.d_model),\n",
    "        )\n",
    "\n",
    "        # Decoder\n",
    "        self.decoder = Decoder(\n",
    "            [\n",
    "                DecoderLayer(\n",
    "                    AttentionLayer(\n",
    "                        FullAttention(\n",
    "                            True,\n",
    "                            configs.factor,\n",
    "                            attention_dropout=configs.dropout,\n",
    "                            output_attention=False,\n",
    "                        ),\n",
    "                        configs.d_model,\n",
    "                        configs.n_heads,\n",
    "                    ),\n",
    "                    AttentionLayer(\n",
    "                        FullAttention(\n",
    "                            False,\n",
    "                            configs.factor,\n",
    "                            attention_dropout=configs.dropout,\n",
    "                            output_attention=False,\n",
    "                        ),\n",
    "                        configs.d_model,\n",
    "                        configs.n_heads,\n",
    "                    ),\n",
    "                    configs.d_model,\n",
    "                    configs.d_ff,\n",
    "                    dropout=configs.dropout,\n",
    "                    activation=configs.activation,\n",
    "                    exp_setting=configs.exp_setting,\n",
    "                )\n",
    "                for _ in range(configs.d_layers)\n",
    "            ],\n",
    "            norm_layer=nn.LayerNorm(configs.d_model),\n",
    "            projection=nn.Linear(configs.d_model, 1, bias=True),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x shape [B, feature_dim]\n",
    "        B, F = x.shape\n",
    "        x_enc = x.unsqueeze(1)  # => [B,1,F]\n",
    "        x_dec = x.unsqueeze(1)\n",
    "        x_mark_enc = torch.zeros(B,1,4, device=x.device)\n",
    "        x_mark_dec = torch.zeros(B,1,4, device=x.device)\n",
    "\n",
    "        enc_out = self.enc_embedding(x_enc, x_mark_enc)\n",
    "        enc_out = self.fan_layer(enc_out)\n",
    "        enc_out, _ = self.encoder(enc_out)\n",
    "\n",
    "        dec_out = self.dec_embedding(x_dec, x_mark_dec)\n",
    "        dec_out = self.decoder(dec_out, enc_out)\n",
    "        return dec_out.squeeze(1).squeeze(-1)  # => [B]\n",
    "\n",
    "###############################\n",
    "# 3) Torch Dataset\n",
    "###############################\n",
    "class ResidualDataset(Dataset):\n",
    "    \"\"\"\n",
    "    We want to predict 'residual' from some chosen input features (like dayofyear, ratio, etc.)\n",
    "    For train samples, y=residual is known. For test samples, y = NaN => we won't use them in training.\n",
    "    \"\"\"\n",
    "    def __init__(self, df, feature_cols, target_col):\n",
    "        # Filter only rows that have a valid residual (train only)\n",
    "        self.df = df[~df[target_col].isna()].copy()\n",
    "        self.X = self.df[feature_cols].values.astype(np.float32)\n",
    "        self.y = self.df[target_col].values.astype(np.float32)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return (\n",
    "            torch.tensor(self.X[idx], dtype=torch.float32),\n",
    "            torch.tensor(self.y[idx], dtype=torch.float32),\n",
    "        )\n",
    "\n",
    "###############################\n",
    "# 4) Main\n",
    "###############################\n",
    "def main():\n",
    "    # 1) Load data\n",
    "    # STEP 1: Load real Kaggle data\n",
    "    TRAIN_CSV = \"playground-series-s5e1/train.csv\"\n",
    "    TEST_CSV  = \"playground-series-s5e1/test.csv\"\n",
    "\n",
    "    train_df = pd.read_csv(TRAIN_CSV)\n",
    "    test_df  = pd.read_csv(TEST_CSV)\n",
    "\n",
    "    # The 'train_df' should have columns: id, country, store, product, date, num_sold\n",
    "    # The 'test_df'  should have columns: id, country, store, product, date\n",
    "\n",
    "    # Convert `date` to datetime and mark test=0/1\n",
    "    train_df['date'] = pd.to_datetime(train_df['date'])\n",
    "    train_df['test'] = 0\n",
    "\n",
    "    test_df['date']  = pd.to_datetime(test_df['date'])\n",
    "    test_df['test']  = 1\n",
    "\n",
    "    # STEP 2: Combine train+test\n",
    "    df = pd.concat([train_df, test_df], ignore_index=True)\n",
    "\n",
    "    # 2) Compute all factors + ratio + residual\n",
    "    df_all = compute_factors(train_df, test_df, CFG)\n",
    "\n",
    "    # 3) Split back into train_df, test_df\n",
    "    train_df = df_all[df_all['test'] == 0].copy()\n",
    "    test_df  = df_all[df_all['test'] == 1].copy()\n",
    "\n",
    "    # ------------------------------------------\n",
    "    # Choose features to feed the FAN model:\n",
    "    # e.g. dayofyear, ratio, weekday, etc.\n",
    "    # You can add more advanced features (sin/cos, holiday flags, etc.)\n",
    "    # ------------------------------------------\n",
    "    feature_cols = ['dayofyear','weekday','ratio']  # minimal example\n",
    "    target_col   = 'residual'\n",
    "\n",
    "    # 4) Build PyTorch Datasets\n",
    "    train_dataset = ResidualDataset(train_df, feature_cols, target_col)\n",
    "    print(\"Training samples:\", len(train_dataset))\n",
    "\n",
    "    # 5) DataLoader\n",
    "    train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True)\n",
    "\n",
    "    # 6) Initialize FAN\n",
    "    torch.cuda.empty_cache()\n",
    "    input_dim = len(feature_cols)\n",
    "    model = FANModel(feature_dim=input_dim, d_model=32)\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    model = model.to(device)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "    criterion = nn.MSELoss()\n",
    "\n",
    "    # 7) Train\n",
    "    epochs = 2\n",
    "    model.train()\n",
    "    for epoch in range(epochs):\n",
    "        epoch_loss = 0\n",
    "        for batch_x, batch_y in train_loader:\n",
    "            batch_x = batch_x.to(device)\n",
    "            batch_y = batch_y.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            pred_y = model(batch_x)\n",
    "            loss = criterion(pred_y, batch_y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            epoch_loss += loss.item()*len(batch_x)\n",
    "        epoch_loss /= len(train_dataset)\n",
    "        print(f\"[Epoch {epoch+1}/{epochs}] MSE Loss: {epoch_loss:.6f}\")\n",
    "\n",
    "    # 8) Inference on test\n",
    "    # Build the features for test\n",
    "    test_X = torch.tensor(test_df[feature_cols].values.astype(np.float32), device=device)\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        pred_residual = model(test_X).cpu().numpy()  # shape [test_size]\n",
    "\n",
    "    # 9) Final prediction = ratio * predicted_residual\n",
    "    test_ratio = test_df['ratio'].values\n",
    "    test_preds = test_ratio * pred_residual\n",
    "\n",
    "    # (Optionally) you can multiply by the 1.06 factor or do rounding, etc.\n",
    "    final_preds = np.round(test_preds).astype(int)\n",
    "\n",
    "    # 10) Build submission\n",
    "    submission = pd.DataFrame({\n",
    "        \"id\": test_df[\"id\"].values,\n",
    "        \"num_sold\": final_preds\n",
    "    })\n",
    "    submission.to_csv(\"playground-series-s5e1/fanforecast/submission.csv\", index=False)\n",
    "    print(\"Saved submission.csv!\")\n",
    "    print(submission.head(5))\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
