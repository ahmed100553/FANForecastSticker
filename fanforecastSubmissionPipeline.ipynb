{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial train shape: (230130, 6)\n",
      "Initial test shape: (98550, 5)\n",
      "Min of num_sold BEFORE transform: 5.0\n",
      "Train after transforms: (230130, 15)\n",
      "X_train: (184104, 14) y_train: (184104,)\n",
      "X_val: (46026, 14) y_val: (46026,)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "##################################\n",
    "# 1) LOAD AND PREPROCESS\n",
    "##################################\n",
    "\n",
    "def load_data():\n",
    "    train = pd.read_csv(\"playground-series-s5e1/train.csv\")\n",
    "    test  = pd.read_csv(\"playground-series-s5e1/test.csv\")\n",
    "    sub   = pd.read_csv(\"playground-series-s5e1/sample_submission.csv\")\n",
    "    return train, test, sub\n",
    "\n",
    "def fill_missing_mean(df):\n",
    "    \"\"\"Fill missing in 'num_sold' with the column mean.\"\"\"\n",
    "    df['num_sold'] = df['num_sold'].fillna(df['num_sold'].mean())\n",
    "    return df\n",
    "\n",
    "def kaggle_transform_num_sold(train):\n",
    "    \"\"\"\n",
    "    1) min–max scale using (x - min) / (max - min)\n",
    "    2) log1p\n",
    "    3) sqrt\n",
    "    4) outlier-clip (IQR)\n",
    "    Returns: transformed train + dictionary of transform stats\n",
    "    \"\"\"\n",
    "\n",
    "    # 1) Compute min, max from the training\n",
    "    num_sold_min = train['num_sold'].min()\n",
    "    num_sold_max = train['num_sold'].max()\n",
    "\n",
    "    # 2) Min–max scale\n",
    "    train['num_sold'] = (train['num_sold'] - num_sold_min) / (num_sold_max - num_sold_min)\n",
    "\n",
    "    # 3) log1p\n",
    "    train['num_sold'] = np.log1p(train['num_sold'])\n",
    "\n",
    "    # 4) sqrt\n",
    "    train['num_sold'] = np.sqrt(train['num_sold'])\n",
    "\n",
    "    # 5) outlier clip with IQR\n",
    "    q1 = train['num_sold'].quantile(0.25)\n",
    "    q3 = train['num_sold'].quantile(0.75)\n",
    "    iqr = q3 - q1\n",
    "    lower = q1 - 1.5*iqr\n",
    "    upper = q3 + 1.5*iqr\n",
    "    train['num_sold'] = train['num_sold'].clip(lower, upper)\n",
    "\n",
    "    # Return transform stats so we can invert later\n",
    "    transform_stats = {\n",
    "        'num_sold_min': num_sold_min,\n",
    "        'num_sold_max': num_sold_max,\n",
    "        'lower_bound': lower,\n",
    "        'upper_bound': upper\n",
    "    }\n",
    "    return train, transform_stats\n",
    "\n",
    "def invert_kaggle_transform(y_pred_transformed, stats):\n",
    "    \"\"\"\n",
    "    Reverse the transformations done by kaggle_transform_num_sold:\n",
    "      1) outlier clip -> we can't truly 'unclip', so we just skip\n",
    "      2) sqrt -> square\n",
    "      3) expm1\n",
    "      4) min–max invert\n",
    "    \"\"\"\n",
    "    # 1) square\n",
    "    y_pred = y_pred_transformed**2\n",
    "\n",
    "    # 2) expm1\n",
    "    y_pred = np.expm1(y_pred)\n",
    "\n",
    "    # 3) invert min–max\n",
    "    num_sold_min = stats['num_sold_min']\n",
    "    num_sold_max = stats['num_sold_max']\n",
    "    y_pred = y_pred * (num_sold_max - num_sold_min) + num_sold_min\n",
    "\n",
    "    return y_pred\n",
    "\n",
    "##################################\n",
    "# 2) CREATE DATASET\n",
    "##################################\n",
    "class KaggleStyleDataset(Dataset):\n",
    "    \"\"\"\n",
    "    We'll treat each row as an independent sample, as the Kaggle example does.\n",
    "    We do one-hot encoding for country, store, product, etc.\n",
    "    X will be all the columns except 'num_sold', y is the transformed 'num_sold'.\n",
    "    \"\"\"\n",
    "    def __init__(self, df):\n",
    "        # df already has 'num_sold' transformed\n",
    "        self.X = df.drop(['num_sold'], axis=1).values.astype(np.float32)\n",
    "        self.y = df['num_sold'].values.astype(np.float32)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return (\n",
    "            torch.tensor(self.X[idx], dtype=torch.float32),\n",
    "            torch.tensor(self.y[idx], dtype=torch.float32)\n",
    "        )\n",
    "\n",
    "##################################\n",
    "# 3) FAN MODEL (Minimal Variation)\n",
    "##################################\n",
    "# We'll reuse your FANTimeSeriesModel, but adapt so it takes X as shape [Batch, Features]\n",
    "# rather than [Batch, seq_len, enc_in]. We'll do a simple fix: treat \"Features\" as \"seq_len\",\n",
    "# or wrap it in a single time step. We'll keep the rest the same just for demonstration.\n",
    "##################################\n",
    "\n",
    "from layers.SelfAttention_Family import FullAttention, AttentionLayer\n",
    "from layers.Transformer_EncDec import Decoder, DecoderLayer, Encoder, EncoderLayer\n",
    "from layers.Embed import DataEmbedding\n",
    "from layers.FANLayer import FANLayer\n",
    "\n",
    "class FANForKaggle(nn.Module):\n",
    "    \"\"\"\n",
    "    Minimal example: we pretend we have seq_len=1, enc_in=FeatureDim\n",
    "    to reuse your FAN architecture. \n",
    "    In reality, if you truly want a tabular approach, you might remove \n",
    "    the 1D convolution, etc. \n",
    "    \"\"\"\n",
    "    def __init__(self, feature_dim=20, d_model=128):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "\n",
    "        # We'll treat 'feature_dim' as enc_in\n",
    "        # But we do a single time-step => [B, 1, feature_dim]\n",
    "        # so freq= 'h', etc. => time features = 4 might not truly apply\n",
    "        # We'll do a minimal approach anyway.\n",
    "        class DummyConfigs:\n",
    "            enc_in = feature_dim\n",
    "            dec_in = feature_dim\n",
    "            c_out = 1\n",
    "            d_model = 128\n",
    "            embed = 'timeF'\n",
    "            freq = 'h'\n",
    "            dropout = 0.1\n",
    "            e_layers = 3\n",
    "            d_layers = 1\n",
    "            d_ff = 512\n",
    "            n_heads = 8\n",
    "            factor = 5\n",
    "            activation = 'gelu'\n",
    "            output_attention = False\n",
    "            pred_len = 1\n",
    "            exp_setting = 0\n",
    "        \n",
    "        configs = DummyConfigs()\n",
    "\n",
    "        self.enc_embedding = DataEmbedding(\n",
    "            configs.enc_in, configs.d_model, configs.embed, configs.freq, configs.dropout\n",
    "        )\n",
    "        # We'll reuse the same embedding for dec. \n",
    "        self.dec_embedding = DataEmbedding(\n",
    "            configs.dec_in, configs.d_model, configs.embed, configs.freq, configs.dropout\n",
    "        )\n",
    "\n",
    "        self.fan_layer = FANLayer(configs.d_model, configs.d_model)\n",
    "\n",
    "        # Encoder\n",
    "        self.encoder = Encoder(\n",
    "            [\n",
    "                EncoderLayer(\n",
    "                    AttentionLayer(\n",
    "                        FullAttention(\n",
    "                            False,\n",
    "                            configs.factor,\n",
    "                            attention_dropout=configs.dropout,\n",
    "                            output_attention=configs.output_attention\n",
    "                        ),\n",
    "                        configs.d_model,\n",
    "                        configs.n_heads,\n",
    "                    ),\n",
    "                    configs.d_model,\n",
    "                    configs.d_ff,\n",
    "                    dropout=configs.dropout,\n",
    "                    activation=configs.activation,\n",
    "                    exp_setting=configs.exp_setting\n",
    "                )\n",
    "                for _ in range(configs.e_layers)\n",
    "            ],\n",
    "            norm_layer=nn.LayerNorm(configs.d_model),\n",
    "        )\n",
    "\n",
    "        # Decoder\n",
    "        self.decoder = Decoder(\n",
    "            [\n",
    "                DecoderLayer(\n",
    "                    AttentionLayer(\n",
    "                        FullAttention(\n",
    "                            True,\n",
    "                            configs.factor,\n",
    "                            attention_dropout=configs.dropout,\n",
    "                            output_attention=False,\n",
    "                        ),\n",
    "                        configs.d_model,\n",
    "                        configs.n_heads,\n",
    "                    ),\n",
    "                    AttentionLayer(\n",
    "                        FullAttention(\n",
    "                            False,\n",
    "                            configs.factor,\n",
    "                            attention_dropout=configs.dropout,\n",
    "                            output_attention=False,\n",
    "                        ),\n",
    "                        configs.d_model,\n",
    "                        configs.n_heads,\n",
    "                    ),\n",
    "                    configs.d_model,\n",
    "                    configs.d_ff,\n",
    "                    dropout=configs.dropout,\n",
    "                    activation=configs.activation,\n",
    "                    exp_setting=configs.exp_setting,\n",
    "                )\n",
    "                for _ in range(configs.d_layers)\n",
    "            ],\n",
    "            norm_layer=nn.LayerNorm(configs.d_model),\n",
    "            projection=nn.Linear(configs.d_model, 1, bias=True),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        x: shape [B, feature_dim]\n",
    "        We'll artificially treat it as: [B, 1, feature_dim] to feed the existing code.\n",
    "        We'll also pass a dummy 'time feature' array for x_mark_enc, x_mark_dec.\n",
    "        \"\"\"\n",
    "        B, F = x.shape\n",
    "        # Expand to [B, 1, F]\n",
    "        x_enc = x.unsqueeze(1)\n",
    "        x_dec = x.unsqueeze(1)  # same input for dec\n",
    "        # Create dummy time features [B, 1, 4]\n",
    "        x_mark_enc = torch.zeros(B, 1, 4, device=x.device)\n",
    "        x_mark_dec = torch.zeros(B, 1, 4, device=x.device)\n",
    "\n",
    "        # pass\n",
    "        enc_out = self.enc_embedding(x_enc, x_mark_enc)\n",
    "        enc_out = self.fan_layer(enc_out)\n",
    "        enc_out, _ = self.encoder(enc_out)\n",
    "\n",
    "        dec_out = self.dec_embedding(x_dec, x_mark_dec)\n",
    "        dec_out = self.decoder(dec_out, enc_out)  # shape [B, 1, 1]\n",
    "        return dec_out.squeeze(1).squeeze(-1)  # => [B]\n",
    "\n",
    "##################################\n",
    "# 4) MAIN PIPELINE\n",
    "##################################\n",
    "\n",
    "def main():\n",
    "    #### A) LOAD\n",
    "    train, test, sub = load_data()\n",
    "    print(\"Initial train shape:\", train.shape)\n",
    "    print(\"Initial test shape:\", test.shape)\n",
    "\n",
    "    #### B) Transform train\n",
    "    train = fill_missing_mean(train)\n",
    "    # We now do all the Kaggle-style transforms on 'num_sold'\n",
    "    # remove date, id or keep them if needed, etc.\n",
    "    # We'll just do it like the Kaggle snippet\n",
    "    min_val = train['num_sold'].min()\n",
    "    print(\"Min of num_sold BEFORE transform:\", min_val)\n",
    "\n",
    "    train, transform_stats = kaggle_transform_num_sold(train)\n",
    "\n",
    "    #### C) One-Hot or get_dummies for country, store, product, etc.\n",
    "    # 1) We'll drop 'id','date' if they exist\n",
    "    if 'id' in train.columns: \n",
    "        train.drop('id', axis=1, inplace=True)\n",
    "    if 'date' in train.columns:\n",
    "        train.drop('date', axis=1, inplace=True)\n",
    "\n",
    "    cat_cols = ['country','store','product']\n",
    "    for c in cat_cols:\n",
    "        if c in train.columns:\n",
    "            train = pd.get_dummies(train, columns=[c])\n",
    "\n",
    "    # train now has numeric columns plus 'num_sold'\n",
    "    # We'll separate X,y => y is the transformed 'num_sold'\n",
    "    # e.g. X => everything except 'num_sold'\n",
    "    # y => 'num_sold'\n",
    "    y = train['num_sold'].values\n",
    "    X = train.drop(['num_sold'], axis=1).values.astype(np.float32)\n",
    "\n",
    "    print(\"Train after transforms:\", train.shape)\n",
    "\n",
    "    # D) Build Dataset\n",
    "    # We'll replicate the Kaggle approach: random or time-based train/val?\n",
    "    # For simplicity, let's do a random split:\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    X_train, X_val, y_train, y_val = train_test_split(\n",
    "        X, y, test_size=0.2, shuffle=True, random_state=42\n",
    "    )\n",
    "    print(\"X_train:\", X_train.shape, \"y_train:\", y_train.shape)\n",
    "    print(\"X_val:\", X_val.shape, \"y_val:\", y_val.shape)\n",
    "\n",
    "    # Torch datasets\n",
    "    class SimpleDataset(Dataset):\n",
    "        def __init__(self, X_, y_):\n",
    "            self.X_ = X_\n",
    "            self.y_ = y_\n",
    "        def __len__(self):\n",
    "            return len(self.X_)\n",
    "        def __getitem__(self, idx):\n",
    "            return (\n",
    "                torch.tensor(self.X_[idx], dtype=torch.float32),\n",
    "                torch.tensor(self.y_[idx], dtype=torch.float32)\n",
    "            )\n",
    "\n",
    "    train_ds = SimpleDataset(X_train, y_train)\n",
    "    val_ds   = SimpleDataset(X_val,   y_val)\n",
    "\n",
    "    train_loader = DataLoader(train_ds, batch_size=32, shuffle=True)\n",
    "    val_loader   = DataLoader(val_ds, batch_size=32, shuffle=False)\n",
    "\n",
    "    # E) Initialize your FAN-based model\n",
    "    feature_dim = X_train.shape[1]  # number of columns\n",
    "    model = FANForKaggle(feature_dim=feature_dim, d_model=64)\n",
    "    model = model.to('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "    optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "    criterion = nn.MSELoss()\n",
    "\n",
    "    #### F) TRAIN\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    model.train()\n",
    "\n",
    "    epochs = 50\n",
    "    optimizer = optim.Adam(model.parameters(), lr=1e-4)\n",
    "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, factor=0.5, patience=3)\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        for batch_x, batch_y in train_loader:\n",
    "            batch_x, batch_y = batch_x.to(device), batch_y.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            # forward\n",
    "            pred = model(batch_x)\n",
    "            loss = criterion(pred, batch_y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss += loss.item()*len(batch_x)\n",
    "        train_loss /= len(train_ds)\n",
    "\n",
    "        # val\n",
    "        from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "\n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "\n",
    "        val_preds = []\n",
    "        val_trues = []\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for batch_x, batch_y in val_loader:\n",
    "                batch_x, batch_y = batch_x.to(device), batch_y.to(device)\n",
    "                \n",
    "                # Forward pass\n",
    "                vpred = model(batch_x)\n",
    "                \n",
    "                # If your model outputs shape [B,1], flatten it\n",
    "                vpred = vpred.view(-1)\n",
    "                \n",
    "                # Compute MSE loss (or your chosen loss)\n",
    "                vloss = criterion(vpred, batch_y)\n",
    "                val_loss += vloss.item() * len(batch_x)\n",
    "                \n",
    "                # Collect predictions & targets for further metrics\n",
    "                val_preds.append(vpred.cpu().numpy())\n",
    "                val_trues.append(batch_y.cpu().numpy())\n",
    "\n",
    "        val_loss /= len(val_ds)\n",
    "        scheduler.step(val_loss)\n",
    "\n",
    "        # Concatenate all batches\n",
    "        val_preds = np.concatenate(val_preds, axis=0)\n",
    "        val_trues = np.concatenate(val_trues, axis=0)\n",
    "\n",
    "        # Compute additional metrics\n",
    "        mse = mean_squared_error(val_trues, val_preds)\n",
    "        rmse = np.sqrt(mse)\n",
    "        mae = mean_absolute_error(val_trues, val_preds)\n",
    "        r2 = r2_score(val_trues, val_preds)\n",
    "        # MAPE needs care if some y-values are zero; handle gracefully\n",
    "        mape = np.mean(np.abs((val_trues - val_preds) / (val_trues + 1e-8))) * 100\n",
    "\n",
    "        print(f\"[Epoch {epoch+1}/{epochs}] \"\n",
    "            f\"train_loss={train_loss:.4f}, val_loss={val_loss:.4f}, \"\n",
    "            f\"MSE={mse:.4f}, RMSE={rmse:.4f}, MAE={mae:.4f}, R2={r2:.4f}, MAPE={mape:.2f}%\")\n",
    "\n",
    "\n",
    "    #### G) PREDICTION for Test\n",
    "    # We'll do the same transforms to test, then invert at the end\n",
    "    # 1) fill missing if any\n",
    "    #test = fill_missing_mean(test)\n",
    "\n",
    "    # 2) min–max scale => log => sqrt => clip\n",
    "    # BUT test has no 'num_sold' to transform. We'll do a dummy approach, or we might just do final predictions?\n",
    "    # The Kaggle example used an LSTM, so it had real feature engineering. \n",
    "    # Actually we only have 'country','store','product' + 'id','date'.\n",
    "\n",
    "    # We'll replicate the get_dummies approach:\n",
    "    if 'id' in test.columns:\n",
    "        test_id = test['id'].values\n",
    "        test.drop('id', axis=1, inplace=True)\n",
    "    if 'date' in test.columns:\n",
    "        test.drop('date', axis=1, inplace=True)\n",
    "\n",
    "    cat_cols2 = ['country','store','product']\n",
    "    for c in cat_cols2:\n",
    "        if c in test.columns:\n",
    "            test = pd.get_dummies(test, columns=[c])\n",
    "\n",
    "    # Ensure test has the same columns as train (besides 'num_sold')\n",
    "    # If train had columns that test lacks, add them:\n",
    "    train_cols = set(train.drop('num_sold', axis=1).columns)\n",
    "    test_cols  = set(test.columns)\n",
    "    missing_in_test = train_cols - test_cols\n",
    "    for mcol in missing_in_test:\n",
    "        test[mcol] = 0\n",
    "\n",
    "    # Possibly test has extra columns => drop them\n",
    "    extra_in_test = test_cols - train_cols\n",
    "    if len(extra_in_test)>0:\n",
    "        test.drop(list(extra_in_test), axis=1, inplace=True)\n",
    "\n",
    "    # reorder columns to match\n",
    "    train_order = list(train.drop('num_sold',axis=1).columns)\n",
    "    test = test[train_order]\n",
    "\n",
    "    # Convert to float32\n",
    "    X_test = test.values.astype(np.float32)\n",
    "\n",
    "    # Predict \n",
    "    test_dataset = SimpleDataset(X_test, np.zeros(len(X_test), dtype=np.float32))\n",
    "    test_loader  = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "    model.eval()\n",
    "    preds_transformed = []\n",
    "    with torch.no_grad():\n",
    "        for bx, _ in test_loader:\n",
    "            bx = bx.to(device)\n",
    "            out = model(bx)\n",
    "            preds_transformed.append(out.cpu().numpy())\n",
    "    preds_transformed = np.concatenate(preds_transformed, axis=0)  # shape (len(test),)\n",
    "\n",
    "    # Invert the Kaggle transform \n",
    "    # i.e. y_pred_original = (expm1( (y_pred^2) ) * (max-min) ) + min\n",
    "    final_preds = invert_kaggle_transform(preds_transformed, transform_stats)\n",
    "\n",
    "    # Build submission\n",
    "    submission = pd.DataFrame({\n",
    "        'id': test_id,\n",
    "        'num_sold': final_preds\n",
    "    })\n",
    "    submission.to_csv(\"submission.csv\", index=False)\n",
    "    print(\"submission.csv created with final predictions!\")\n",
    "\n",
    "\n",
    "if __name__==\"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".conda",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
