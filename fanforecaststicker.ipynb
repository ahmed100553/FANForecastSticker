{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import os\n",
    "\n",
    "##########################################\n",
    "# 1) LOADING + SCALING + GROUPING\n",
    "##########################################\n",
    "\n",
    "def load_data():\n",
    "    train = pd.read_csv(\"playground-series-s5e1/train.csv\")\n",
    "    test  = pd.read_csv(\"playground-series-s5e1/test.csv\")\n",
    "    sub   = pd.read_csv(\"playground-series-s5e1/sample_submission.csv\")\n",
    "    return train, test, sub\n",
    "\n",
    "def fill_missing_mean(df):\n",
    "    \"\"\"Fill missing in 'num_sold' with mean (only for train).\"\"\"\n",
    "    df['num_sold'] = df['num_sold'].fillna(df['num_sold'].mean())\n",
    "    return df\n",
    "\n",
    "def kaggle_transform_num_sold(df):\n",
    "    \"\"\"\n",
    "    Applies min–max -> log1p -> sqrt -> IQR clip to `num_sold`.\n",
    "    Returns (df, transform_stats) so we can invert it later.\n",
    "    \"\"\"\n",
    "    num_sold_min = df['num_sold'].min()\n",
    "    num_sold_max = df['num_sold'].max()\n",
    "\n",
    "    # 1) min–max\n",
    "    df['num_sold'] = (df['num_sold'] - num_sold_min) / (num_sold_max - num_sold_min)\n",
    "\n",
    "    # 2) log1p\n",
    "    df['num_sold'] = np.log1p(df['num_sold'])\n",
    "\n",
    "    # 3) sqrt\n",
    "    df['num_sold'] = np.sqrt(df['num_sold'])\n",
    "\n",
    "    # 4) IQR clip\n",
    "    q1 = df['num_sold'].quantile(0.25)\n",
    "    q3 = df['num_sold'].quantile(0.75)\n",
    "    iqr = q3 - q1\n",
    "    lower = q1 - 1.5 * iqr\n",
    "    upper = q3 + 1.5 * iqr\n",
    "    df['num_sold'] = df['num_sold'].clip(lower, upper)\n",
    "\n",
    "    transform_stats = {\n",
    "        'num_sold_min': num_sold_min,\n",
    "        'num_sold_max': num_sold_max,\n",
    "        'lower_bound': lower,\n",
    "        'upper_bound': upper\n",
    "    }\n",
    "    return df, transform_stats\n",
    "\n",
    "def invert_kaggle_transform(y_pred_transformed, stats):\n",
    "    \"\"\"\n",
    "    Invert the transform: square -> expm1 -> invert min–max.\n",
    "    \"\"\"\n",
    "    # 1) square\n",
    "    y_pred = y_pred_transformed**2\n",
    "    # 2) expm1\n",
    "    y_pred = np.expm1(y_pred)\n",
    "    # 3) min–max invert\n",
    "    num_sold_min = stats['num_sold_min']\n",
    "    num_sold_max = stats['num_sold_max']\n",
    "    y_pred = y_pred * (num_sold_max - num_sold_min) + num_sold_min\n",
    "    return y_pred\n",
    "\n",
    "\n",
    "##########################################\n",
    "# 2) BUILD SLIDING WINDOWS BY GROUP\n",
    "##########################################\n",
    "def build_group_windows(group_df, seq_len=96, pred_len=1):\n",
    "    \"\"\"\n",
    "    group_df: columns [date, country, store, product, num_sold, ...]\n",
    "    We'll assume only 'num_sold' is used for the time-series input.\n",
    "    Return lists of x_seq, y_seq for each sliding window in this group.\n",
    "    Single-step approach: x_seq => last 96 days, y_seq => next day.\n",
    "    \"\"\"\n",
    "    group_df = group_df.sort_values('date')  # sort by date\n",
    "    arr = group_df['num_sold'].values  # shape (N,)\n",
    "\n",
    "    x_list = []\n",
    "    y_list = []\n",
    "    # We'll also store the corresponding date of the label so we can track it if needed\n",
    "    # but for pure training, it's not mandatory. We'll omit for brevity.\n",
    "\n",
    "    N = len(arr)\n",
    "    for i in range(N - seq_len - pred_len + 1):\n",
    "        x_seq = arr[i : i+seq_len]                 # last 96\n",
    "        y_seq = arr[i+seq_len : i+seq_len+pred_len]# next day\n",
    "        x_list.append(x_seq)\n",
    "        y_list.append(y_seq[0])  # single-step => just 1 value\n",
    "\n",
    "    return x_list, y_list\n",
    "\n",
    "class MultiSeriesDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Collects sliding windows from all (country, store, product) groups.\n",
    "    \"\"\"\n",
    "    def __init__(self, train_df, seq_len=96):\n",
    "        self.seq_len = seq_len\n",
    "        # group by (country, store, product)\n",
    "        self.samples = []\n",
    "        gdf = train_df.groupby(['country','store','product'], as_index=False)\n",
    "\n",
    "        for (coun,st,prod), subdf in gdf:\n",
    "            x_list, y_list = build_group_windows(subdf, seq_len=seq_len, pred_len=1)\n",
    "            for x_seq, y_val in zip(x_list, y_list):\n",
    "                self.samples.append((x_seq, y_val))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        x_seq, y_val = self.samples[idx]\n",
    "        x_seq = torch.tensor(x_seq, dtype=torch.float32)  # shape [seq_len]\n",
    "        y_val = torch.tensor(y_val, dtype=torch.float32)\n",
    "        return x_seq, y_val\n",
    "\n",
    "\n",
    "##########################################\n",
    "# 3) FAN MODEL\n",
    "##########################################\n",
    "# We'll do a minimal approach: input is [B, seq_len], we pretend that's [B, seq_len, 1].\n",
    "# Then we do the same FAN-based Transformer approach over time steps.\n",
    "from layers.SelfAttention_Family import FullAttention, AttentionLayer\n",
    "from layers.Transformer_EncDec import Decoder, DecoderLayer, Encoder, EncoderLayer\n",
    "from layers.Embed import DataEmbedding\n",
    "from layers.FANLayer import FANLayer\n",
    "\n",
    "class FANTimeSeries(nn.Module):\n",
    "    \"\"\"\n",
    "    A multi-step Transformer for a single time-series input dimension (or we do enc_in=1).\n",
    "    We interpret x: [B, seq_len] -> [B, seq_len, 1].\n",
    "    \"\"\"\n",
    "    def __init__(self, seq_len=96, d_model=128, e_layers=3, d_layers=1):\n",
    "        super().__init__()\n",
    "        class DummyConfigs:\n",
    "            enc_in = 1\n",
    "            dec_in = 1\n",
    "            c_out = 1\n",
    "            d_model = 128\n",
    "            embed = 'timeF'\n",
    "            freq = 'h'\n",
    "            dropout = 0.1\n",
    "            e_layers = 3\n",
    "            d_layers = 1\n",
    "            d_ff = 512\n",
    "            n_heads = 8\n",
    "            factor = 5\n",
    "            activation = 'gelu'\n",
    "            output_attention = False\n",
    "            pred_len = 1\n",
    "            exp_setting = 0\n",
    "        configs = DummyConfigs()\n",
    "\n",
    "        self.pred_len = configs.pred_len\n",
    "        self.enc_embedding = DataEmbedding(1, configs.d_model, configs.embed, configs.freq, configs.dropout)\n",
    "        self.dec_embedding = DataEmbedding(1, configs.d_model, configs.embed, configs.freq, configs.dropout)\n",
    "        self.fan_layer = FANLayer(configs.d_model, configs.d_model)\n",
    "\n",
    "        # Encoder\n",
    "        self.encoder = Encoder(\n",
    "            [\n",
    "                EncoderLayer(\n",
    "                    AttentionLayer(\n",
    "                        FullAttention(False, configs.factor,\n",
    "                                      attention_dropout=configs.dropout,\n",
    "                                      output_attention=configs.output_attention),\n",
    "                        configs.d_model,\n",
    "                        configs.n_heads),\n",
    "                    configs.d_model,\n",
    "                    configs.d_ff,\n",
    "                    dropout=configs.dropout,\n",
    "                    activation=configs.activation,\n",
    "                    exp_setting=configs.exp_setting\n",
    "                )\n",
    "                for _ in range(configs.e_layers)\n",
    "            ],\n",
    "            norm_layer=nn.LayerNorm(configs.d_model),\n",
    "        )\n",
    "\n",
    "        # Decoder\n",
    "        self.decoder = Decoder(\n",
    "            [\n",
    "                DecoderLayer(\n",
    "                    AttentionLayer(\n",
    "                        FullAttention(True, configs.factor, attention_dropout=configs.dropout, output_attention=False),\n",
    "                        configs.d_model,\n",
    "                        configs.n_heads,\n",
    "                    ),\n",
    "                    AttentionLayer(\n",
    "                        FullAttention(False, configs.factor, attention_dropout=configs.dropout, output_attention=False),\n",
    "                        configs.d_model,\n",
    "                        configs.n_heads,\n",
    "                    ),\n",
    "                    configs.d_model,\n",
    "                    configs.d_ff,\n",
    "                    dropout=configs.dropout,\n",
    "                    activation=configs.activation,\n",
    "                    exp_setting=configs.exp_setting,\n",
    "                )\n",
    "                for _ in range(configs.d_layers)\n",
    "            ],\n",
    "            norm_layer=nn.LayerNorm(configs.d_model),\n",
    "            projection=nn.Linear(configs.d_model, 1, bias=True),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        x: shape [B, seq_len], single feature -> interpret as [B, seq_len, 1].\n",
    "        We'll do a single-step decode => x_dec shape also [B, 1, 1].\n",
    "        \"\"\"\n",
    "        device = x.device\n",
    "        B, L = x.shape\n",
    "        x_enc = x.unsqueeze(-1)  # [B, L, 1]\n",
    "        # We'll create a single \"start token\" or zero for the decoder\n",
    "        x_dec = torch.zeros([B, 1, 1], device=device)\n",
    "\n",
    "        # dummy time features\n",
    "        x_mark_enc = torch.zeros([B, L, 4], device=device)\n",
    "        x_mark_dec = torch.zeros([B, 1, 4], device=device)\n",
    "\n",
    "        enc_out = self.enc_embedding(x_enc, x_mark_enc)  # [B,L,d_model]\n",
    "        enc_out = self.fan_layer(enc_out)\n",
    "        enc_out, _ = self.encoder(enc_out)\n",
    "\n",
    "        dec_out = self.dec_embedding(x_dec, x_mark_dec)\n",
    "        dec_out = self.decoder(dec_out, enc_out)  # shape [B,1,1]\n",
    "        # => single-step forecast\n",
    "        return dec_out.squeeze(1).squeeze(-1)  # [B]\n",
    "\n",
    "##########################################\n",
    "# 4) MAIN PIPELINE\n",
    "##########################################\n",
    "def main():\n",
    "    # A) Load\n",
    "    train, test, sub = load_data()\n",
    "    train['date'] = pd.to_datetime(train['date'])\n",
    "    test['date']  = pd.to_datetime(test['date'])\n",
    "\n",
    "    # B) Preprocess\n",
    "    # fill missing in train\n",
    "    train = fill_missing_mean(train)\n",
    "\n",
    "    # C) Apply \"Kaggle transform\" to train's num_sold\n",
    "    train, transform_stats = kaggle_transform_num_sold(train)\n",
    "\n",
    "    # D) We'll keep date, country, store, product, but numeric for grouping & sliding windows\n",
    "    # Sort train by (country, store, product, date)\n",
    "    train = train.sort_values(['country','store','product','date'])\n",
    "\n",
    "    # E) Build multi-series dataset with sliding windows\n",
    "    seq_len = 96\n",
    "    train_ds = MultiSeriesDataset(train, seq_len=seq_len)\n",
    "    train_loader = DataLoader(train_ds, batch_size=32, shuffle=True)\n",
    "\n",
    "    # F) Initialize model\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model = FANTimeSeries(seq_len=seq_len, d_model=64, e_layers=2, d_layers=1).to(device)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=1e-4)\n",
    "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, factor=0.5, patience=3)\n",
    "    criterion = nn.MSELoss()\n",
    "\n",
    "    # G) Train loop\n",
    "    epochs = 50\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        for x_seq, y_val in train_loader:\n",
    "            x_seq, y_val = x_seq.to(device), y_val.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            pred = model(x_seq)  # shape [B]\n",
    "            loss = criterion(pred, y_val)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item() * len(x_seq)\n",
    "        total_loss /= len(train_ds)\n",
    "        print(f\"Epoch {epoch+1}/{epochs}, train_loss={total_loss:.4f}\")\n",
    "        scheduler.step(total_loss)\n",
    "    # H) Save model\n",
    "    torch.save(model.state_dict(), \"fan_model_timeseries.pth\")\n",
    "    print(\"Model saved.\")\n",
    "\n",
    "    # I) Build a dictionary of historical data for each group, including the train portion\n",
    "    # for easy inference\n",
    "    # We'll store *transformed* num_sold in a structure so we can keep rolling forward\n",
    "    history = {}\n",
    "    for (c,s,p), gdf in train.groupby([\"country\",\"store\",\"product\"]):\n",
    "        gdf = gdf.sort_values(\"date\")\n",
    "        history[(c,s,p)] = list(gdf['num_sold'].values)  # store as a list of transformed values\n",
    "\n",
    "    # J) Inference on test\n",
    "    # We'll sort test by date and do a rolling approach: for each row => build the last 96 from that group\n",
    "    test = test.sort_values([\"country\",\"store\",\"product\",\"date\"])\n",
    "\n",
    "    predictions = []\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for i, row in test.iterrows():\n",
    "            c = row['country']\n",
    "            s = row['store']\n",
    "            p = row['product']\n",
    "            # retrieve that group's history\n",
    "            if (c,s,p) not in history:\n",
    "                history[(c,s,p)] = []  # if group not in train, start empty\n",
    "\n",
    "            group_hist = history[(c,s,p)]\n",
    "\n",
    "            # Build input x_seq\n",
    "            if len(group_hist) < seq_len:\n",
    "                # pad with zeros if not enough history\n",
    "                padded = [0.0]*(seq_len - len(group_hist)) + group_hist\n",
    "                x_seq = np.array(padded[-seq_len:], dtype=np.float32)\n",
    "            else:\n",
    "                x_seq = np.array(group_hist[-seq_len:], dtype=np.float32)\n",
    "\n",
    "            x_seq_ten = torch.tensor([x_seq], device=device)  # shape [1, seq_len]\n",
    "            pred_t = model(x_seq_ten)  # shape [1]\n",
    "            pred_val = pred_t.item()   # transformed\n",
    "            # store it into group_hist for future windows\n",
    "            group_hist.append(pred_val)\n",
    "\n",
    "            # invert\n",
    "            num_sold_pred = invert_kaggle_transform(np.array([pred_val], dtype=np.float32), transform_stats)[0]\n",
    "            predictions.append(num_sold_pred)\n",
    "\n",
    "    # K) Build submission\n",
    "    submission = pd.DataFrame({\n",
    "        'id': test['id'],\n",
    "        'num_sold': predictions\n",
    "    })\n",
    "    submission.to_csv(\"submission.csv\", index=False)\n",
    "    print(\"submission.csv created!\")\n",
    "\n",
    "\n",
    "if __name__==\"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "databundleVersionId": 10652996,
     "sourceId": 85723,
     "sourceType": "competition"
    },
    {
     "datasetId": 6501797,
     "sourceId": 10501562,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 6501798,
     "sourceId": 10501567,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30839,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": ".conda",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
