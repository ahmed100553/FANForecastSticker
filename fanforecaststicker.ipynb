{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "###################################\n",
    "# 1) IMPORTS\n",
    "###################################\n",
    "import os\n",
    "import sys\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import math\n",
    "from layers.Transformer_EncDec import Decoder, DecoderLayer, Encoder, EncoderLayer\n",
    "from layers.SelfAttention_Family import FullAttention, AttentionLayer\n",
    "from layers.Embed import DataEmbedding, PositionalEmbedding, TokenEmbedding, TimeFeatureEmbedding\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "\n",
    "\n",
    "###################################\n",
    "# 2) LOADING THE DATA\n",
    "###################################\n",
    "def load_data():\n",
    "    # Adjust paths as needed if they are in a different directory\n",
    "    train = pd.read_csv(\"playground-series-s5e1/train.csv\")\n",
    "    test = pd.read_csv(\"playground-series-s5e1/test.csv\")\n",
    "    submission = pd.read_csv(\"playground-series-s5e1/sample_submission.csv\")\n",
    "    return train, test, submission\n",
    "\n",
    "###################################\n",
    "# 3) BASIC PREPROCESS & IMPUTATION\n",
    "###################################\n",
    "def preprocess_data(train: pd.DataFrame):\n",
    "    # Convert date to datetime\n",
    "    train['date'] = pd.to_datetime(train['date'])\n",
    "\n",
    "    # Impute missing num_sold by group means (country, product, store, month)\n",
    "    train['month'] = train['date'].dt.month\n",
    "    group_cols = ['country','product','store','month']\n",
    "    group_means = train.groupby(group_cols)['num_sold'].transform('mean')\n",
    "    train['num_sold'] = train['num_sold'].fillna(group_means)\n",
    "\n",
    "    # Fill any remaining NaNs with overall mean\n",
    "    train['num_sold'].fillna(train['num_sold'].mean(), inplace=True)\n",
    "    return train\n",
    "\n",
    "###################################\n",
    "# 4) AGGREGATE INTO A SINGLE SERIES\n",
    "###################################\n",
    "def aggregate_timeseries(train: pd.DataFrame):\n",
    "    \"\"\"\n",
    "    Aggregate all (country,store,product) combos into one daily series\n",
    "    Return a DataFrame with [date, num_sold] plus time features\n",
    "    \"\"\"\n",
    "    agg_df = train.groupby('date', as_index=False)['num_sold'].sum().sort_values('date')\n",
    "\n",
    "    # Create 4 time features so freq='h' in TimeFeatureEmbedding matches (month, day, weekday, year_mod)\n",
    "    agg_df['month']   = agg_df['date'].dt.month\n",
    "    agg_df['day']     = agg_df['date'].dt.day\n",
    "    agg_df['weekday'] = agg_df['date'].dt.weekday\n",
    "    agg_df['year']    = agg_df['date'].dt.year\n",
    "    agg_df['year_mod'] = agg_df['year'] - 2000\n",
    "\n",
    "    return agg_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "###################################\n",
    "# 5) TRAIN/VALID SPLIT BY DATE\n",
    "###################################\n",
    "def split_data_by_date(agg_df: pd.DataFrame, date_str=\"2015-01-01\"):\n",
    "    \"\"\"\n",
    "    Splits into train/valid based on date_str boundary\n",
    "    \"\"\"\n",
    "    split_date = pd.to_datetime(date_str)\n",
    "    train_mask = agg_df['date'] < split_date\n",
    "    valid_mask = agg_df['date'] >= split_date\n",
    "\n",
    "    train_data = agg_df[train_mask].reset_index(drop=True)\n",
    "    valid_data = agg_df[valid_mask].reset_index(drop=True)\n",
    "    return train_data, valid_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "###################################\n",
    "# 6) MAKE WINDOWS (SLIDING)\n",
    "###################################\n",
    "def make_windows(data, time_feat, input_size=96, pred_size=24):\n",
    "    \"\"\"\n",
    "    data: 1D array of the target\n",
    "    time_feat: 2D array of shape (N, 4) containing [month, day, weekday, year_mod]\n",
    "    Return: x_enc, x_mark_enc, x_dec, x_mark_dec, y arrays\n",
    "    \"\"\"\n",
    "    x_enc_list, x_mark_enc_list = [], []\n",
    "    x_dec_list, x_mark_dec_list = [], []\n",
    "    y_list = []\n",
    "    L = len(data)\n",
    "    for i in range(L - input_size - pred_size):\n",
    "        x_enc = data[i : i+input_size]\n",
    "        t_enc = time_feat[i : i+input_size]\n",
    "        x_dec = data[i+input_size : i+input_size+pred_size]\n",
    "        t_dec = time_feat[i+input_size : i+input_size+pred_size]\n",
    "        y    = data[i+input_size : i+input_size+pred_size]\n",
    "\n",
    "        x_enc_list.append(x_enc)\n",
    "        x_mark_enc_list.append(t_enc)\n",
    "        x_dec_list.append(x_dec)\n",
    "        x_mark_dec_list.append(t_dec)\n",
    "        y_list.append(y)\n",
    "\n",
    "    return (\n",
    "        np.array(x_enc_list),\n",
    "        np.array(x_mark_enc_list),\n",
    "        np.array(x_dec_list),\n",
    "        np.array(x_mark_dec_list),\n",
    "        np.array(y_list)\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "###################################\n",
    "# 7) CUSTOM DATASET\n",
    "###################################\n",
    "class TimeSeriesWindowDataset(Dataset):\n",
    "    def __init__(self, x_enc, x_mark_enc, x_dec, x_mark_dec, y):\n",
    "        super().__init__()\n",
    "        self.x_enc = x_enc\n",
    "        self.x_mark_enc = x_mark_enc\n",
    "        self.x_dec = x_dec\n",
    "        self.x_mark_dec = x_mark_dec\n",
    "        self.y = y\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.x_enc)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        # Convert to torch and return\n",
    "        # x_enc, x_dec => (Length, 1)\n",
    "        return (\n",
    "            torch.tensor(self.x_enc[idx],      dtype=torch.float32),\n",
    "            torch.tensor(self.x_mark_enc[idx], dtype=torch.float32),\n",
    "            torch.tensor(self.x_dec[idx],      dtype=torch.float32),\n",
    "            torch.tensor(self.x_mark_dec[idx], dtype=torch.float32),\n",
    "            torch.tensor(self.y[idx],          dtype=torch.float32)\n",
    "        )\n",
    "\n",
    "###################################\n",
    "# 8) EMBEDDING + FAN LAYER + TRANSFORMER\n",
    "###################################\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# --- (B) FAN Layer\n",
    "class FANLayer(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim, bias=True, with_gate=True):\n",
    "        super(FANLayer, self).__init__()\n",
    "        self.input_linear_p = nn.Linear(input_dim, output_dim//4, bias=bias)\n",
    "        self.input_linear_g = nn.Linear(input_dim, (output_dim - output_dim//2))\n",
    "        self.activation = nn.GELU()\n",
    "        if with_gate:\n",
    "            self.gate = nn.Parameter(torch.randn(1, dtype=torch.float32))\n",
    "\n",
    "    def forward(self, src):\n",
    "        # src: [B, L, d_model]\n",
    "        g = self.activation(self.input_linear_g(src))  # => [B, L, out_dim - out_dim//2]\n",
    "        p = self.input_linear_p(src)                   # => [B, L, out_dim//4]\n",
    "        if not hasattr(self, 'gate'):\n",
    "            output = torch.cat((torch.cos(p), torch.sin(p), g), dim=-1)\n",
    "        else:\n",
    "            gate = torch.sigmoid(self.gate)\n",
    "            output = torch.cat((gate*torch.cos(p), gate*torch.sin(p), (1-gate)*g), dim=-1)\n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "###################################\n",
    "# 9) FANTimeSeriesModel (Transformer + FANLayer)\n",
    "###################################\n",
    "class FANTimeSeriesModel(nn.Module):\n",
    "    \"\"\"\n",
    "    Transformer-based model for time-series forecasting with FAN layer integration.\n",
    "    \"\"\"\n",
    "    def __init__(self, configs):\n",
    "        super(FANTimeSeriesModel, self).__init__()\n",
    "        self.pred_len = configs.pred_len\n",
    "        self.output_attention = configs.output_attention\n",
    "\n",
    "        # Embedding\n",
    "        self.enc_embedding = DataEmbedding(\n",
    "            configs.enc_in, configs.d_model, configs.embed, configs.freq, configs.dropout\n",
    "        )\n",
    "        self.dec_embedding = DataEmbedding(\n",
    "            configs.dec_in, configs.d_model, configs.embed, configs.freq, configs.dropout\n",
    "        )\n",
    "\n",
    "        # FAN Layer\n",
    "        self.fan_layer = FANLayer(configs.d_model, configs.d_model)\n",
    "\n",
    "        # Encoder\n",
    "        self.encoder = Encoder(\n",
    "            [\n",
    "                EncoderLayer(\n",
    "                    AttentionLayer(\n",
    "                        FullAttention(\n",
    "                            False, configs.factor,\n",
    "                            attention_dropout=configs.dropout,\n",
    "                            output_attention=configs.output_attention,\n",
    "                        ),\n",
    "                        configs.d_model,\n",
    "                        configs.n_heads,\n",
    "                    ),\n",
    "                    configs.d_model,\n",
    "                    configs.d_ff,\n",
    "                    dropout=configs.dropout,\n",
    "                    activation=configs.activation,\n",
    "                    exp_setting=configs.exp_setting,\n",
    "                )\n",
    "                for _ in range(configs.e_layers)\n",
    "            ],\n",
    "            norm_layer=nn.LayerNorm(configs.d_model),\n",
    "        )\n",
    "\n",
    "        # Decoder\n",
    "        self.decoder = Decoder(\n",
    "            [\n",
    "                DecoderLayer(\n",
    "                    AttentionLayer(\n",
    "                        FullAttention(\n",
    "                            True, configs.factor,\n",
    "                            attention_dropout=configs.dropout,\n",
    "                            output_attention=False,\n",
    "                        ),\n",
    "                        configs.d_model,\n",
    "                        configs.n_heads,\n",
    "                    ),\n",
    "                    AttentionLayer(\n",
    "                        FullAttention(\n",
    "                            False, configs.factor,\n",
    "                            attention_dropout=configs.dropout,\n",
    "                            output_attention=False,\n",
    "                        ),\n",
    "                        configs.d_model,\n",
    "                        configs.n_heads,\n",
    "                    ),\n",
    "                    configs.d_model,\n",
    "                    configs.d_ff,\n",
    "                    dropout=configs.dropout,\n",
    "                    activation=configs.activation,\n",
    "                    exp_setting=configs.exp_setting,\n",
    "                )\n",
    "                for _ in range(configs.d_layers)\n",
    "            ],\n",
    "            norm_layer=nn.LayerNorm(configs.d_model),\n",
    "            projection=nn.Linear(configs.d_model, configs.c_out, bias=True),\n",
    "        )\n",
    "\n",
    "    def forward(self, x_enc, x_mark_enc, x_dec, x_mark_dec,\n",
    "                enc_self_mask=None, dec_self_mask=None, dec_enc_mask=None):\n",
    "        \"\"\"\n",
    "        x_enc:      [B, seq_len, enc_in]\n",
    "        x_mark_enc: [B, seq_len, 4]\n",
    "        x_dec:      [B, pred_len, dec_in]\n",
    "        x_mark_dec: [B, pred_len, 4]\n",
    "        \"\"\"\n",
    "        #print(f\"x_enc shape: {x_enc.shape}\")\n",
    "        #print(f\"x_mark_enc shape: {x_mark_enc.shape}\")\n",
    "        enc_out = self.enc_embedding(x_enc, x_mark_enc)\n",
    "        #print(f\"enc_out shape after embedding: {enc_out.shape}\")\n",
    "\n",
    "        #print(f\"x_dec shape: {x_dec.shape}\")\n",
    "        #print(f\"x_mark_dec shape: {x_mark_dec.shape}\")\n",
    "        dec_out = self.dec_embedding(x_dec, x_mark_dec)\n",
    "        #print(f\"dec_out shape after embedding: {dec_out.shape}\")\n",
    "\n",
    "        # FAN layer\n",
    "        enc_out = self.fan_layer(enc_out)\n",
    "        #print(f\"enc_out shape after FAN layer: {enc_out.shape}\")\n",
    "\n",
    "        # Encoder\n",
    "        enc_out, attns = self.encoder(enc_out, attn_mask=enc_self_mask)\n",
    "        #print(f\"enc_out shape after encoder: {enc_out.shape}\")\n",
    "\n",
    "        # Decoder\n",
    "        dec_out = self.decoder(dec_out, enc_out, x_mask=dec_self_mask, cross_mask=dec_enc_mask)\n",
    "        #print(f\"dec_out shape after decoder: {dec_out.shape}\")\n",
    "\n",
    "        if self.output_attention:\n",
    "            return dec_out[:, -self.pred_len:, :], attns\n",
    "        else:\n",
    "            return dec_out[:, -self.pred_len:, :]\n",
    "\n",
    "###################################\n",
    "# 10) CONFIGS\n",
    "###################################\n",
    "class Configs:\n",
    "    # Because we have only one aggregated series, we set enc_in=1 and dec_in=1.\n",
    "    enc_in = 1\n",
    "    dec_in = 1\n",
    "    c_out = 1\n",
    "    d_model = 64\n",
    "    embed = 'timeF'\n",
    "    freq = 'h'        # We'll keep 'h' so we have 4 time features in x_mark\n",
    "    dropout = 0.1\n",
    "    e_layers = 2\n",
    "    d_layers = 1\n",
    "    d_ff = 256\n",
    "    n_heads = 8\n",
    "    factor = 5\n",
    "    activation = 'gelu'\n",
    "    output_attention = False\n",
    "    pred_len = 24\n",
    "    exp_setting = 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    # 1) Load Data\n",
    "    train, test, submission = load_data()\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "    # 2) Preprocess/impute\n",
    "    train = preprocess_data(train)\n",
    "\n",
    "    # 3) Aggregate daily\n",
    "    agg_df = aggregate_timeseries(train)\n",
    "\n",
    "    # 4) Split train/valid (ending at 2016-12-31 for example)\n",
    "    train_data, valid_data = split_data_by_date(agg_df, date_str=\"2017-01-01\")\n",
    "    #   Now train_data covers up to 2016-12-31. valid_data is 2017+ (but you might or might not use it)\n",
    "\n",
    "    # 5) Build arrays\n",
    "    y_train = train_data['num_sold'].values  # aggregator from 2010–2016\n",
    "    dates_train = train_data['date'].values  # keep the matching dates\n",
    "    \n",
    "    # 6) Log transform & scale the target\n",
    "    #    log1p(x) = log(1 + x), handles zeros gracefully, reduces explosive growth\n",
    "    y_train_log = np.log1p(y_train)  # shape (N,)\n",
    "\n",
    "    scaler = StandardScaler()\n",
    "    # We'll scale the log values instead of the raw aggregator\n",
    "    y_train_log_scaled = scaler.fit_transform(y_train_log.reshape(-1,1)).ravel()\n",
    "    \n",
    "    # 7) Time features for training\n",
    "    time_feat_train = train_data[['month','day','weekday','year_mod']].values\n",
    "\n",
    "    # 8) Build windows\n",
    "    input_size = 96\n",
    "    pred_size = 24\n",
    "    x_enc_tr, x_mark_enc_tr, x_dec_tr, x_mark_dec_tr, y_tr = make_windows(\n",
    "        y_train_log_scaled, time_feat_train, input_size, pred_size\n",
    "    )\n",
    "    # ^ now y_tr is also in log-scale (scaled)\n",
    "\n",
    "    # 9) Reshape (N, length, 1)\n",
    "    x_enc_tr = x_enc_tr[..., None]\n",
    "    x_dec_tr = x_dec_tr[..., None]\n",
    "\n",
    "    # 10) Dataset, loader\n",
    "    train_dataset = TimeSeriesWindowDataset(\n",
    "        x_enc_tr, x_mark_enc_tr, x_dec_tr, x_mark_dec_tr, y_tr\n",
    "    )\n",
    "    train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "    # 11) Model\n",
    "    configs = Configs()\n",
    "    model = FANTimeSeriesModel(configs).to(device)\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "    # 12) Train loop (short version)\n",
    "    num_epochs = 50\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        for batch_data in train_loader:\n",
    "            x_enc, x_mark_enc, x_dec, x_mark_dec, y_true_log_scaled = [bd.to(device) for bd in batch_data]\n",
    "            optimizer.zero_grad()\n",
    "            y_pred_log_scaled = model(x_enc, x_mark_enc, x_dec, x_mark_dec).squeeze(-1)\n",
    "            loss = criterion(y_pred_log_scaled, y_true_log_scaled)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item() * len(x_enc)\n",
    "        train_loss = total_loss / len(train_dataset)\n",
    "        print(f\"[Epoch {epoch+1}/{num_epochs}] Train Loss: {train_loss:.4f}\")\n",
    "\n",
    "    # 13) Save model weights\n",
    "    torch.save(model.state_dict(), \"fan_model.pth\")\n",
    "    print(\"Saved model weights to fan_model.pth\")\n",
    "\n",
    "    # 14) Also save arrays & scaler\n",
    "    #     We'll store the *log-scaled* aggregator so we can continue from that point in forecast_future\n",
    "    np.save(\"dates_train.npy\", dates_train)                         # shape (N,)\n",
    "    np.save(\"y_train_scaled.npy\", y_train_log_scaled)              # log-scale, scaled aggregator\n",
    "    np.save(\"time_feat_train.npy\", time_feat_train)                # shape (N,4)\n",
    "\n",
    "    import joblib\n",
    "    joblib.dump(scaler, \"scaler.pkl\")\n",
    "    print(\"Saved aggregator log-scaler & data.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forecast_future_aggregator(start_date=\"2017-01-01\", end_date=\"2019-12-31\"):\n",
    "    \"\"\"\n",
    "    1) Load aggregator data & model from disk\n",
    "    2) Iteratively forecast daily aggregator from start_date to end_date in chunks of pred_len=24 days\n",
    "    3) Return a DataFrame with [date, aggregator_pred_original]\n",
    "    \"\"\"\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    # A) Load previously saved aggregator data\n",
    "    dates_train = np.load(\"dates_train.npy\", allow_pickle=True)  # shape (N,)\n",
    "    y_train_scaled = np.load(\"y_train_scaled.npy\", allow_pickle=True)  # shape (N,)\n",
    "    time_feat_train = np.load(\"time_feat_train.npy\", allow_pickle=True)  # shape (N,4)\n",
    "    \n",
    "    # B) Load model & scaler\n",
    "    configs = Configs()\n",
    "    model = FANTimeSeriesModel(configs).to(device)\n",
    "    model.load_state_dict(torch.load(\"fan_model.pth\", map_location=device))\n",
    "    model.eval()\n",
    "\n",
    "    import joblib\n",
    "    scaler = joblib.load(\"scaler.pkl\")\n",
    "    \n",
    "    input_size = configs.pred_len*4  # or 96 as you set in training\n",
    "    pred_size  = configs.pred_len    # 24\n",
    "\n",
    "    # For clarity, ensure these match what you used in main:\n",
    "    input_size = 96\n",
    "    pred_size  = 24\n",
    "\n",
    "    # C) Build a \"full\" aggregator history array that we can extend\n",
    "    #    For now, let's store them in lists, then convert to numpy\n",
    "    full_dates = list(dates_train)          # up to 2016-12-31\n",
    "    full_scaled = list(y_train_scaled)      # aggregator scaled\n",
    "    full_timefeat = list(time_feat_train)   # shape (N,4), but store as list of arrays\n",
    "\n",
    "    # Convert start_date & end_date to datetime\n",
    "    start_dt = pd.to_datetime(start_date)\n",
    "    end_dt   = pd.to_datetime(end_date)\n",
    "\n",
    "    # We'll forecast in increments of 24 days until end_dt\n",
    "    current_forecast_start = start_dt\n",
    "\n",
    "    # We'll keep a \"future\" array of date-> aggregator\n",
    "    # But we store it in full_dates / full_scaled / full_timefeat\n",
    "    while True:\n",
    "        # 1) If current_forecast_start > end_dt, break\n",
    "        if current_forecast_start > end_dt:\n",
    "            break\n",
    "\n",
    "        # 2) Our next chunk end\n",
    "        chunk_end = current_forecast_start + pd.Timedelta(days=pred_size - 1)\n",
    "        if chunk_end > end_dt:\n",
    "            chunk_end = end_dt\n",
    "            # we might have a partial chunk if there's fewer than 24 days left\n",
    "            chunk_size = (chunk_end - current_forecast_start).days + 1\n",
    "        else:\n",
    "            chunk_size = pred_size\n",
    "\n",
    "        # 3) We'll take the last input_size days from the aggregator as x_enc\n",
    "        #    We do that from full_scaled\n",
    "        #    Note we must ensure we have at least input_size in full_scaled\n",
    "        if len(full_scaled) < input_size:\n",
    "            raise ValueError(\"Not enough history to do the first forecast! Check code.\")\n",
    "        \n",
    "        # Slice out the last input_size from full_scaled/timefeat\n",
    "        x_enc_arr = np.array(full_scaled[-input_size:])  # shape (96,)\n",
    "        x_mark_enc_arr = np.array(full_timefeat[-input_size:])  # shape (96,4)\n",
    "\n",
    "        # Next chunk of future dates\n",
    "        future_dates_chunk = pd.date_range(current_forecast_start, periods=chunk_size, freq='D')\n",
    "        \n",
    "        # Build time features for that future chunk\n",
    "        # shape => (chunk_size,4)\n",
    "        t_dec_list = []\n",
    "        for dt in future_dates_chunk:\n",
    "            month   = dt.month\n",
    "            day     = dt.day\n",
    "            weekday = dt.weekday()\n",
    "            year_mod= dt.year - 2000\n",
    "            t_dec_list.append([month, day, weekday, year_mod])\n",
    "        x_mark_dec_arr = np.array(t_dec_list, dtype=np.float32)  # (24,4) or partial\n",
    "\n",
    "        # For x_dec, we don't have \"known future\" aggregator if we're purely forecasting.\n",
    "        # Usually we feed zeros or just an empty input. We'll feed zeros:\n",
    "        x_dec_arr = np.zeros((chunk_size,), dtype=np.float32)\n",
    "\n",
    "        # 4) Reshape to 3D for model\n",
    "        x_enc = torch.tensor(x_enc_arr[None, :, None], device=device, dtype=torch.float32)  # [1,96,1]\n",
    "        x_mark_enc = torch.tensor(x_mark_enc_arr[None,:,:], device=device, dtype=torch.float32)  # [1,96,4]\n",
    "        x_dec = torch.tensor(x_dec_arr[None, :, None], device=device, dtype=torch.float32)  # [1,chunk_size,1]\n",
    "        x_mark_dec = torch.tensor(x_mark_dec_arr[None,:,:], device=device, dtype=torch.float32) # [1,chunk_size,4]\n",
    "\n",
    "        with torch.no_grad():\n",
    "            pred = model(x_enc, x_mark_enc, x_dec, x_mark_dec)\n",
    "            # pred shape => [1, chunk_size, 1]\n",
    "            pred = pred.squeeze(0).squeeze(-1).cpu().numpy()  # => shape (chunk_size,)\n",
    "\n",
    "        # 5) Append these forecast values to full_scaled/timefeat\n",
    "        for i in range(chunk_size):\n",
    "            full_dates.append(future_dates_chunk[i])\n",
    "            full_scaled.append(pred[i])\n",
    "            full_timefeat.append(x_mark_dec_arr[i])\n",
    "\n",
    "        # 6) Move the forecast start forward by chunk_size days\n",
    "        current_forecast_start = chunk_end + pd.Timedelta(days=1)\n",
    "        \n",
    "        # If chunk_end == end_dt, we are done\n",
    "        if chunk_end >= end_dt:\n",
    "            break\n",
    "\n",
    "    # Now full_scaled/timefeat includes the entire aggregator from 2010-01-01 to 2019-12-31\n",
    "    # The portion from index [len(y_train_scaled): ] => 2017-01-01 to 2019-12-31 is forecast\n",
    "\n",
    "    # Convert to arrays\n",
    "    full_dates = np.array(full_dates)\n",
    "    full_scaled = np.array(full_scaled)\n",
    "\n",
    "    # D) Build a dataframe for the entire period\n",
    "    df_full = pd.DataFrame({\n",
    "        'date': full_dates,\n",
    "        'aggregator_scaled': full_scaled\n",
    "    })\n",
    "\n",
    "    # E) Invert scaling for aggregator\n",
    "    aggregator_original = scaler.inverse_transform(full_scaled.reshape(-1,1)).ravel()\n",
    "    df_full['aggregator_original'] = aggregator_original\n",
    "\n",
    "    # Finally, filter only the forecast range (>= 2017-01-01)\n",
    "    df_forecast = df_full[df_full['date'] >= start_dt].copy()\n",
    "    df_forecast = df_forecast[df_forecast['date'] <= end_dt]\n",
    "\n",
    "    return df_forecast[['date','aggregator_original']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_submission():\n",
    "    # 1) Forecast aggregator from 2017-01-01 to 2019-12-31\n",
    "    df_forecast = forecast_future_aggregator(start_date=\"2017-01-01\", end_date=\"2019-12-31\")\n",
    "    # => columns: [date, aggregator_original]\n",
    "\n",
    "    # 2) Load test.csv\n",
    "    test = pd.read_csv(\"playground-series-s5e1/test.csv\")\n",
    "    test['date'] = pd.to_datetime(test['date'])\n",
    "\n",
    "    # 3) Merge test with df_forecast on 'date'\n",
    "    #    So each test row gets aggregator_original for that date\n",
    "    merged = pd.merge(test, df_forecast, how='left', on='date')\n",
    "    # merged => columns: ['id','date','country','store','product','aggregator_original']\n",
    "\n",
    "    # 4) The single-series aggregator forecast is \"num_sold\" for each row\n",
    "    #    Because the model doesn't differentiate country/store/product\n",
    "    merged.rename(columns={'aggregator_original': 'num_sold'}, inplace=True)\n",
    "\n",
    "    # 5) Output the required columns in Kaggle format: 'id','num_sold'\n",
    "    submission = merged[['id','num_sold']].copy()\n",
    "\n",
    "    # If you have any missing predictions outside the forecast range, fill with something\n",
    "    submission['num_sold'] = submission['num_sold'].fillna(0)  # or an average\n",
    "\n",
    "    # 6) Save\n",
    "    submission.to_csv(\"submission.csv\", index=False)\n",
    "    print(\"submission.csv created!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ahmed\\AppData\\Local\\Temp\\ipykernel_38132\\3936641927.py:50: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  train['num_sold'].fillna(train['num_sold'].mean(), inplace=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 1/50] Train Loss: 0.2547\n",
      "[Epoch 2/50] Train Loss: 0.0640\n",
      "[Epoch 3/50] Train Loss: 0.0481\n",
      "[Epoch 4/50] Train Loss: 0.0390\n",
      "[Epoch 5/50] Train Loss: 0.0334\n",
      "[Epoch 6/50] Train Loss: 0.0289\n",
      "[Epoch 7/50] Train Loss: 0.0269\n",
      "[Epoch 8/50] Train Loss: 0.0237\n",
      "[Epoch 9/50] Train Loss: 0.0211\n",
      "[Epoch 10/50] Train Loss: 0.0199\n",
      "[Epoch 11/50] Train Loss: 0.0184\n",
      "[Epoch 12/50] Train Loss: 0.0168\n",
      "[Epoch 13/50] Train Loss: 0.0168\n",
      "[Epoch 14/50] Train Loss: 0.0150\n",
      "[Epoch 15/50] Train Loss: 0.0142\n",
      "[Epoch 16/50] Train Loss: 0.0136\n",
      "[Epoch 17/50] Train Loss: 0.0125\n",
      "[Epoch 18/50] Train Loss: 0.0120\n",
      "[Epoch 19/50] Train Loss: 0.0114\n",
      "[Epoch 20/50] Train Loss: 0.0105\n",
      "[Epoch 21/50] Train Loss: 0.0101\n",
      "[Epoch 22/50] Train Loss: 0.0098\n",
      "[Epoch 23/50] Train Loss: 0.0092\n",
      "[Epoch 24/50] Train Loss: 0.0088\n",
      "[Epoch 25/50] Train Loss: 0.0084\n",
      "[Epoch 26/50] Train Loss: 0.0083\n",
      "[Epoch 27/50] Train Loss: 0.0077\n",
      "[Epoch 28/50] Train Loss: 0.0078\n",
      "[Epoch 29/50] Train Loss: 0.0074\n",
      "[Epoch 30/50] Train Loss: 0.0070\n",
      "[Epoch 31/50] Train Loss: 0.0067\n",
      "[Epoch 32/50] Train Loss: 0.0065\n",
      "[Epoch 33/50] Train Loss: 0.0062\n",
      "[Epoch 34/50] Train Loss: 0.0063\n",
      "[Epoch 35/50] Train Loss: 0.0058\n",
      "[Epoch 36/50] Train Loss: 0.0056\n",
      "[Epoch 37/50] Train Loss: 0.0054\n",
      "[Epoch 38/50] Train Loss: 0.0052\n",
      "[Epoch 39/50] Train Loss: 0.0052\n",
      "[Epoch 40/50] Train Loss: 0.0051\n",
      "[Epoch 41/50] Train Loss: 0.0049\n",
      "[Epoch 42/50] Train Loss: 0.0047\n",
      "[Epoch 43/50] Train Loss: 0.0046\n",
      "[Epoch 44/50] Train Loss: 0.0045\n",
      "[Epoch 45/50] Train Loss: 0.0045\n",
      "[Epoch 46/50] Train Loss: 0.0043\n",
      "[Epoch 47/50] Train Loss: 0.0042\n",
      "[Epoch 48/50] Train Loss: 0.0042\n",
      "[Epoch 49/50] Train Loss: 0.0041\n",
      "[Epoch 50/50] Train Loss: 0.0039\n",
      "Saved model weights to fan_model.pth\n",
      "Saved aggregator log-scaler & data.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ahmed\\AppData\\Local\\Temp\\ipykernel_38132\\2419542846.py:17: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(\"fan_model.pth\", map_location=device))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "submission.csv created!\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # 1) Train aggregator model and save everything\n",
    "    main()\n",
    "\n",
    "    # 2) Create the submission\n",
    "    create_submission()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "configs = Configs()\n",
    "model = FANTimeSeriesModel(configs).to(device)\n",
    "    model.load_state_dict(torch.load(\"fan_model.pth\", map_location=device))\n",
    "    model.eval()\n",
    "\n",
    "    import joblib\n",
    "    scaler = joblib.load(\"scaler.pkl\")\n",
    "    \n",
    "    input_size = configs.pred_len*4  # or 96 as you set in training\n",
    "    pred_size  = configs.pred_len    # 24\n",
    "\n",
    "    # For clarity, ensure these match what you used in main:\n",
    "    input_size = 96\n",
    "    pred_size  = 24\n",
    "\n",
    "    # C) Build a \"full\" aggregator history array that we can extend\n",
    "    #    For now, let's store them in lists, then convert to numpy\n",
    "    full_dates = list(dates_train)          # up to 2016-12-31\n",
    "    full_scaled = list(y_train_scaled)      # aggregator scaled\n",
    "    full_timefeat = list(time_feat_train)   # shape (N,4), but store as list of arrays\n",
    "\n",
    "    # Convert start_date & end_date to datetime\n",
    "    start_dt = pd.to_datetime(start_date)\n",
    "    end_dt   = pd.to_datetime(end_date)\n",
    "\n",
    "    # We'll forecast in increments of 24 days until end_dt\n",
    "    current_forecast_start = start_dt\n",
    "\n",
    "    # We'll keep a \"future\" array of date-> aggregator\n",
    "    # But we store it in full_dates / full_scaled / full_timefeat\n",
    "    while True:\n",
    "        # 1) If current_forecast_start > end_dt, break\n",
    "        if current_forecast_start > end_dt:\n",
    "            break\n",
    "\n",
    "        # 2) Our next chunk end\n",
    "        chunk_end = current_forecast_start + pd.Timedelta(days=pred_size - 1)\n",
    "        if chunk_end > end_dt:\n",
    "            chunk_end = end_dt\n",
    "            # we might have a partial chunk if there's fewer than 24 days left\n",
    "            chunk_size = (chunk_end - current_forecast_start).days + 1\n",
    "        else:\n",
    "            chunk_size = pred_size\n",
    "\n",
    "        # 3) We'll take the last input_size days from the aggregator as x_enc\n",
    "        #    We do that from full_scaled\n",
    "        #    Note we must ensure we have at least input_size in full_scaled\n",
    "        if len(full_scaled) < input_size:\n",
    "            raise ValueError(\"Not enough history to do the first forecast! Check code.\")\n",
    "        \n",
    "        # Slice out the last input_size from full_scaled/timefeat\n",
    "        x_enc_arr = np.array(full_scaled[-input_size:])  # shape (96,)\n",
    "        x_mark_enc_arr = np.array(full_timefeat[-input_size:])  # shape (96,4)\n",
    "\n",
    "        # Next chunk of future dates\n",
    "        future_dates_chunk = pd.date_range(current_forecast_start, periods=chunk_size, freq='D')\n",
    "        \n",
    "        # Build time features for that future chunk\n",
    "        # shape => (chunk_size,4)\n",
    "        t_dec_list = []\n",
    "        for dt in future_dates_chunk:\n",
    "            month   = dt.month\n",
    "            day     = dt.day\n",
    "            weekday = dt.weekday()\n",
    "            year_mod= dt.year - 2000\n",
    "            t_dec_list.append([month, day, weekday, year_mod])\n",
    "        x_mark_dec_arr = np.array(t_dec_list, dtype=np.float32)  # (24,4) or partial\n",
    "\n",
    "        # For x_dec, we don't have \"known future\" aggregator if we're purely forecasting.\n",
    "        # Usually we feed zeros or just an empty input. We'll feed zeros:\n",
    "        x_dec_arr = np.zeros((chunk_size,), dtype=np.float32)\n",
    "\n",
    "        # 4) Reshape to 3D for model\n",
    "        x_enc = torch.tensor(x_enc_arr[None, :, None], device=device, dtype=torch.float32)  # [1,96,1]\n",
    "        x_mark_enc = torch.tensor(x_mark_enc_arr[None,:,:], device=device, dtype=torch.float32)  # [1,96,4]\n",
    "        x_dec = torch.tensor(x_dec_arr[None, :, None], device=device, dtype=torch.float32)  # [1,chunk_size,1]\n",
    "        x_mark_dec = torch.tensor(x_mark_dec_arr[None,:,:], device=device, dtype=torch.float32) # [1,chunk_size,4]\n",
    "\n",
    "        with torch.no_grad():\n",
    "            pred = model(x_enc, x_mark_enc, x_dec, x_mark_dec)\n",
    "            # pred shape => [1, chunk_size, 1]\n",
    "            pred = pred.squeeze(0).squeeze(-1).cpu().numpy()  # => shape (chunk_size,)\n",
    "\n",
    "        # 5) Append these forecast values to full_scaled/timefeat\n",
    "        for i in range(chunk_size):\n",
    "            full_dates.append(future_dates_chunk[i])\n",
    "            full_scaled.append(pred[i])\n",
    "            full_timefeat.append(x_mark_dec_arr[i])\n",
    "\n",
    "        # 6) Move the forecast start forward by chunk_size days\n",
    "        current_forecast_start = chunk_end + pd.Timedelta(days=1)\n",
    "        \n",
    "        # If chunk_end == end_dt, we are done\n",
    "        if chunk_end >= end_dt:\n",
    "            break\n",
    "\n",
    "    # Now full_scaled/timefeat includes the entire aggregator from 2010-01-01 to 2019-12-31\n",
    "    # The portion from index [len(y_train_scaled): ] => 2017-01-01 to 2019-12-31 is forecast\n",
    "\n",
    "    # Convert to arrays\n",
    "    full_dates = np.array(full_dates)\n",
    "    full_scaled = np.array(full_scaled)\n",
    "\n",
    "    # D) Build a dataframe for the entire period\n",
    "    df_full = pd.DataFrame({\n",
    "        'date': full_dates,\n",
    "        'aggregator_scaled': full_scaled\n",
    "    })\n",
    "\n",
    "    # E) Invert scaling for aggregator\n",
    "    aggregator_original = scaler.inverse_transform(full_scaled.reshape(-1,1)).ravel()\n",
    "    df_full['aggregator_original'] = aggregator_original\n",
    "\n",
    "    # Finally, filter only the forecast range (>= 2017-01-01)\n",
    "    df_forecast = df_full[df_full['date'] >= start_dt].copy()\n",
    "    df_forecast = df_forecast[df_forecast['date'] <= end_dt]\n",
    "\n",
    "    return df_forecast[['date','aggregator_original']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = FANTimeSeriesModel(configs).to(device)\n",
    "    model.load_state_dict(torch.load(\"fan_model.pth\", map_location=device))\n",
    "    model.eval()\n",
    "\n",
    "    import joblib\n",
    "    scaler = joblib.load(\"scaler.pkl\")\n",
    "    \n",
    "    input_size = configs.pred_len*4  # or 96 as you set in training\n",
    "    pred_size  = configs.pred_len    # 24\n",
    "\n",
    "    # For clarity, ensure these match what you used in main:\n",
    "    input_size = 96\n",
    "    pred_size  = 24\n",
    "\n",
    "    # C) Build a \"full\" aggregator history array that we can extend\n",
    "    #    For now, let's store them in lists, then convert to numpy\n",
    "    full_dates = list(dates_train)          # up to 2016-12-31\n",
    "    full_scaled = list(y_train_scaled)      # aggregator scaled\n",
    "    full_timefeat = list(time_feat_train)   # shape (N,4), but store as list of arrays\n",
    "\n",
    "    # Convert start_date & end_date to datetime\n",
    "    start_dt = pd.to_datetime(start_date)\n",
    "    end_dt   = pd.to_datetime(end_date)\n",
    "\n",
    "    # We'll forecast in increments of 24 days until end_dt\n",
    "    current_forecast_start = start_dt\n",
    "\n",
    "    # We'll keep a \"future\" array of date-> aggregator\n",
    "    # But we store it in full_dates / full_scaled / full_timefeat\n",
    "    while True:\n",
    "        # 1) If current_forecast_start > end_dt, break\n",
    "        if current_forecast_start > end_dt:\n",
    "            break\n",
    "\n",
    "        # 2) Our next chunk end\n",
    "        chunk_end = current_forecast_start + pd.Timedelta(days=pred_size - 1)\n",
    "        if chunk_end > end_dt:\n",
    "            chunk_end = end_dt\n",
    "            # we might have a partial chunk if there's fewer than 24 days left\n",
    "            chunk_size = (chunk_end - current_forecast_start).days + 1\n",
    "        else:\n",
    "            chunk_size = pred_size\n",
    "\n",
    "        # 3) We'll take the last input_size days from the aggregator as x_enc\n",
    "        #    We do that from full_scaled\n",
    "        #    Note we must ensure we have at least input_size in full_scaled\n",
    "        if len(full_scaled) < input_size:\n",
    "            raise ValueError(\"Not enough history to do the first forecast! Check code.\")\n",
    "        \n",
    "        # Slice out the last input_size from full_scaled/timefeat\n",
    "        x_enc_arr = np.array(full_scaled[-input_size:])  # shape (96,)\n",
    "        x_mark_enc_arr = np.array(full_timefeat[-input_size:])  # shape (96,4)\n",
    "\n",
    "        # Next chunk of future dates\n",
    "        future_dates_chunk = pd.date_range(current_forecast_start, periods=chunk_size, freq='D')\n",
    "        \n",
    "        # Build time features for that future chunk\n",
    "        # shape => (chunk_size,4)\n",
    "        t_dec_list = []\n",
    "        for dt in future_dates_chunk:\n",
    "            month   = dt.month\n",
    "            day     = dt.day\n",
    "            weekday = dt.weekday()\n",
    "            year_mod= dt.year - 2000\n",
    "            t_dec_list.append([month, day, weekday, year_mod])\n",
    "        x_mark_dec_arr = np.array(t_dec_list, dtype=np.float32)  # (24,4) or partial\n",
    "\n",
    "        # For x_dec, we don't have \"known future\" aggregator if we're purely forecasting.\n",
    "        # Usually we feed zeros or just an empty input. We'll feed zeros:\n",
    "        x_dec_arr = np.zeros((chunk_size,), dtype=np.float32)\n",
    "\n",
    "        # 4) Reshape to 3D for model\n",
    "        x_enc = torch.tensor(x_enc_arr[None, :, None], device=device, dtype=torch.float32)  # [1,96,1]\n",
    "        x_mark_enc = torch.tensor(x_mark_enc_arr[None,:,:], device=device, dtype=torch.float32)  # [1,96,4]\n",
    "        x_dec = torch.tensor(x_dec_arr[None, :, None], device=device, dtype=torch.float32)  # [1,chunk_size,1]\n",
    "        x_mark_dec = torch.tensor(x_mark_dec_arr[None,:,:], device=device, dtype=torch.float32) # [1,chunk_size,4]\n",
    "\n",
    "        with torch.no_grad():\n",
    "            pred = model(x_enc, x_mark_enc, x_dec, x_mark_dec)\n",
    "            # pred shape => [1, chunk_size, 1]\n",
    "            pred = pred.squeeze(0).squeeze(-1).cpu().numpy()  # => shape (chunk_size,)\n",
    "\n",
    "        # 5) Append these forecast values to full_scaled/timefeat\n",
    "        for i in range(chunk_size):\n",
    "            full_dates.append(future_dates_chunk[i])\n",
    "            full_scaled.append(pred[i])\n",
    "            full_timefeat.append(x_mark_dec_arr[i])\n",
    "\n",
    "        # 6) Move the forecast start forward by chunk_size days\n",
    "        current_forecast_start = chunk_end + pd.Timedelta(days=1)\n",
    "        \n",
    "        # If chunk_end == end_dt, we are done\n",
    "        if chunk_end >= end_dt:\n",
    "            break\n",
    "\n",
    "    # Now full_scaled/timefeat includes the entire aggregator from 2010-01-01 to 2019-12-31\n",
    "    # The portion from index [len(y_train_scaled): ] => 2017-01-01 to 2019-12-31 is forecast\n",
    "\n",
    "    # Convert to arrays\n",
    "    full_dates = np.array(full_dates)\n",
    "    full_scaled = np.array(full_scaled)\n",
    "\n",
    "    # D) Build a dataframe for the entire period\n",
    "    df_full = pd.DataFrame({\n",
    "        'date': full_dates,\n",
    "        'aggregator_scaled': full_scaled\n",
    "    })\n",
    "\n",
    "    # E) Invert scaling for aggregator\n",
    "    aggregator_original = scaler.inverse_transform(full_scaled.reshape(-1,1)).ravel()\n",
    "    df_full['aggregator_original'] = aggregator_original\n",
    "\n",
    "    # Finally, filter only the forecast range (>= 2017-01-01)\n",
    "    df_forecast = df_full[df_full['date'] >= start_dt].copy()\n",
    "    df_forecast = df_forecast[df_forecast['date'] <= end_dt]\n",
    "\n",
    "    return df_forecast[['date','aggregator_original']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "databundleVersionId": 10652996,
     "sourceId": 85723,
     "sourceType": "competition"
    },
    {
     "datasetId": 6501797,
     "sourceId": 10501562,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 6501798,
     "sourceId": 10501567,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30839,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": ".conda",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
